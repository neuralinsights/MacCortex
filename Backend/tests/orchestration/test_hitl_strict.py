"""
MacCortex Human-in-the-Loop ä¸¥æ ¼æµ‹è¯•å¥—ä»¶

æµ‹è¯•çœŸå®åœºæ™¯ã€è¾¹ç¼˜æƒ…å†µã€é”™è¯¯å¤„ç†ã€‚
"""

import pytest
import tempfile
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command

from src.orchestration.swarm_graph import create_full_swarm_graph
from src.orchestration.state import create_initial_state
from src.orchestration.hitl import HITLHelper, RiskAssessor


@pytest.fixture
def tmp_path():
    """ä¸´æ—¶å·¥ä½œç©ºé—´"""
    return Path(tempfile.mkdtemp())


class TestHITLEdgeCases:
    """æµ‹è¯•è¾¹ç¼˜æƒ…å†µ"""

    def test_parse_invalid_user_decision(self):
        """æµ‹è¯•æ— æ•ˆç”¨æˆ·å†³ç­–è¾“å…¥"""
        with pytest.raises(ValueError) as exc_info:
            HITLHelper.parse_user_decision("xyz", "tool_execution")

        assert "æ— æ•ˆçš„å†³ç­–" in str(exc_info.value)
        assert "xyz" in str(exc_info.value)

    def test_parse_user_decision_case_insensitive(self):
        """æµ‹è¯•ç”¨æˆ·å†³ç­–å¤§å°å†™ä¸æ•æ„Ÿ"""
        # æµ‹è¯•å„ç§å¤§å°å†™ç»„åˆ
        test_cases = [
            ("APPROVE", "approve"),
            ("Approve", "approve"),
            ("YES", "approve"),
            ("Yes", "approve"),
            ("DENY", "deny"),
            ("Deny", "deny"),
            ("NO", "deny"),
            ("ABORT", "abort"),
            ("Abort", "abort"),
        ]

        for user_input, expected_action in test_cases:
            decision = HITLHelper.parse_user_decision(user_input, "tool_execution")
            assert decision["action"] == expected_action, f"Failed for input: {user_input}"

    def test_parse_user_decision_aliases(self):
        """æµ‹è¯•ç”¨æˆ·å†³ç­–åˆ«å"""
        # approve åˆ«å
        for alias in ["approve", "yes", "y", "ok"]:
            decision = HITLHelper.parse_user_decision(alias, "tool_execution")
            assert decision["action"] == "approve"

        # deny åˆ«å
        for alias in ["deny", "no", "n", "skip"]:
            decision = HITLHelper.parse_user_decision(alias, "tool_execution")
            assert decision["action"] == "deny"

        # modify åˆ«å
        for alias in ["modify", "edit", "m"]:
            decision = HITLHelper.parse_user_decision(alias, "tool_execution")
            assert decision["action"] == "modify"

        # abort åˆ«å
        for alias in ["abort", "cancel", "stop"]:
            decision = HITLHelper.parse_user_decision(alias, "tool_execution")
            assert decision["action"] == "abort"


class TestRiskAssessment:
    """æµ‹è¯•é£é™©è¯„ä¼°"""

    def test_high_risk_tools(self):
        """æµ‹è¯•é«˜é£é™©å·¥å…·è¯†åˆ«"""
        high_risk_tools = [
            "delete_file",
            "remove_directory",
            "execute_shell",
            "write_database",
            "send_email",
            "make_api_call",
        ]

        for tool_name in high_risk_tools:
            risk = RiskAssessor.assess_tool_risk(tool_name, {})
            assert risk == "high", f"{tool_name} should be high risk"

    def test_medium_risk_tools(self):
        """æµ‹è¯•ä¸­é£é™©å·¥å…·è¯†åˆ«"""
        medium_risk_tools = [
            "write_file",
            "create_directory",
            "move_file",
            "copy_file",
        ]

        for tool_name in medium_risk_tools:
            risk = RiskAssessor.assess_tool_risk(tool_name, {})
            assert risk == "medium", f"{tool_name} should be medium risk"

    def test_low_risk_tools(self):
        """æµ‹è¯•ä½é£é™©å·¥å…·è¯†åˆ«"""
        low_risk_tools = [
            "read_file",
            "list_directory",
            "search_web",
            "unknown_tool",
        ]

        for tool_name in low_risk_tools:
            risk = RiskAssessor.assess_tool_risk(tool_name, {})
            assert risk == "low", f"{tool_name} should be low risk"

    def test_sensitive_path_escalation(self):
        """æµ‹è¯•æ•æ„Ÿè·¯å¾„å‡çº§é£é™©ç­‰çº§"""
        # write_file é€šå¸¸æ˜¯ mediumï¼Œä½†æ¶‰åŠæ•æ„Ÿè·¯å¾„åº”å‡çº§ä¸º high
        sensitive_paths = [
            "/etc/passwd",
            "/usr/bin/sudo",
            "/System/Library/CoreServices/Finder.app",
            "~/.ssh/id_rsa",
            "~/.aws/credentials",
            "/var/log/system.log",
            "C:\\Windows\\System32\\drivers",
        ]

        for path in sensitive_paths:
            risk = RiskAssessor.assess_tool_risk(
                "write_file",
                {"path": path}
            )
            assert risk == "high", f"Writing to {path} should be high risk"

    def test_code_risk_dangerous_keywords(self):
        """æµ‹è¯•ä»£ç é£é™©è¯„ä¼° - å±é™©å…³é”®è¯"""
        dangerous_code_samples = [
            "import os; os.system('rm -rf /')",
            "exec('malicious code')",
            "eval(user_input)",
            "subprocess.popen('shutdown now')",
            "DELETE FROM users;",
            "DROP TABLE accounts;",
            "os.unlink('/etc/passwd')",
            "shutil.rmtree('/var')",
        ]

        for code in dangerous_code_samples:
            risk = RiskAssessor.assess_code_risk(code, "python")
            assert risk == "high", f"Code with dangerous keywords should be high risk: {code[:50]}"

    def test_code_risk_long_code(self):
        """æµ‹è¯•ä»£ç é£é™©è¯„ä¼° - è¶…é•¿ä»£ç """
        # è¶…è¿‡ 1000 å­—ç¬¦çš„ä»£ç åº”è¯¥æ˜¯ medium é£é™©
        long_code = "print('hello')\n" * 100  # ~1400 å­—ç¬¦

        risk = RiskAssessor.assess_code_risk(long_code, "python")
        assert risk == "medium", "Long code should be medium risk"

    def test_code_risk_safe_code(self):
        """æµ‹è¯•ä»£ç é£é™©è¯„ä¼° - å®‰å…¨ä»£ç """
        safe_code = """
def add(a, b):
    return a + b

result = add(1, 2)
print(result)
"""

        risk = RiskAssessor.assess_code_risk(safe_code, "python")
        assert risk == "low", "Safe code should be low risk"


class TestHITLHelperFormatting:
    """æµ‹è¯• HITL æ¶ˆæ¯æ ¼å¼åŒ–"""

    def test_format_tool_execution_interrupt(self):
        """æµ‹è¯•å·¥å…·æ‰§è¡Œä¸­æ–­æ¶ˆæ¯æ ¼å¼åŒ–"""
        prompt_data = {
            "operation": "tool_execution",
            "risk_level": "medium",
            "details": {
                "tool_name": "write_file",
                "subtask_description": "åˆ›å»ºé…ç½®æ–‡ä»¶",
                "tool_args": {
                    "path": "/tmp/config.json",
                    "content": '{"debug": true}'
                }
            },
            "timestamp": "2026-01-22T10:00:00Z",
            "available_actions": ["approve", "deny", "modify", "abort"]
        }

        message = HITLHelper.format_interrupt_message(prompt_data)

        # éªŒè¯å…³é”®ä¿¡æ¯å­˜åœ¨
        assert "tool_execution" in message
        assert "MEDIUM" in message
        assert "write_file" in message
        assert "åˆ›å»ºé…ç½®æ–‡ä»¶" in message
        assert "/tmp/config.json" in message
        assert "approve, deny, modify, abort" in message
        assert "ğŸŸ¡" in message  # medium é£é™©æ ‡è®°

    def test_format_code_generation_interrupt(self):
        """æµ‹è¯•ä»£ç ç”Ÿæˆä¸­æ–­æ¶ˆæ¯æ ¼å¼åŒ–"""
        code = "def hello():\n    print('Hello, World!')"

        prompt_data = {
            "operation": "code_generation",
            "risk_level": "low",
            "details": {
                "language": "python",
                "subtask_description": "ç”Ÿæˆé—®å€™å‡½æ•°",
                "file_path": "/tmp/hello.py",
                "code": code
            },
            "timestamp": "2026-01-22T10:00:00Z",
            "available_actions": ["approve", "regenerate", "modify", "abort"]
        }

        message = HITLHelper.format_interrupt_message(prompt_data)

        # éªŒè¯å…³é”®ä¿¡æ¯å­˜åœ¨
        assert "code_generation" in message
        assert "LOW" in message
        assert "python" in message
        assert "ç”Ÿæˆé—®å€™å‡½æ•°" in message
        assert "/tmp/hello.py" in message
        assert "def hello()" in message or "..." in message  # ä»£ç é¢„è§ˆ
        assert "ğŸŸ¢" in message  # low é£é™©æ ‡è®°

    def test_format_review_intervention_interrupt(self):
        """æµ‹è¯•å®¡æŸ¥ä»‹å…¥ä¸­æ–­æ¶ˆæ¯æ ¼å¼åŒ–"""
        prompt_data = {
            "operation": "review_intervention",
            "risk_level": "medium",
            "details": {
                "subtask_description": "ä¿®å¤è¯­æ³•é”™è¯¯",
                "iteration_count": 2,
                "max_iterations": 3,
                "reviewer_feedback": "ç¼ºå°‘ç±»å‹æ³¨è§£"
            },
            "timestamp": "2026-01-22T10:00:00Z",
            "available_actions": ["continue", "fix_manually", "skip", "abort"]
        }

        message = HITLHelper.format_interrupt_message(prompt_data)

        # éªŒè¯å…³é”®ä¿¡æ¯å­˜åœ¨
        assert "review_intervention" in message
        assert "ä¿®å¤è¯­æ³•é”™è¯¯" in message
        assert "2/3" in message  # è¿­ä»£æ¬¡æ•°
        assert "ç¼ºå°‘ç±»å‹æ³¨è§£" in message
        assert "continue, fix_manually, skip, abort" in message


@pytest.mark.asyncio
class TestHITLModifyOperation:
    """æµ‹è¯• modify æ“ä½œ"""

    async def test_modify_tool_args(self, tmp_path):
        """æµ‹è¯•ä¿®æ”¹å·¥å…·å‚æ•°"""

        def create_mock_llm():
            mock_llm = AsyncMock()

            # Planner å“åº”
            planner_response = Mock()
            planner_response.content = f"""```json
{{
  "task": "åˆ›å»ºæ–‡ä»¶",
  "subtasks": [
    {{
      "id": "task-1",
      "type": "tool",
      "description": "åˆ›å»ºæµ‹è¯•æ–‡ä»¶",
      "dependencies": [],
      "acceptance_criteria": ["æ–‡ä»¶åˆ›å»ºæˆåŠŸ"],
      "tool_name": "write_file",
      "tool_args": {{
        "path": "{tmp_path}/original.txt",
        "content": "Original Content"
      }}
    }}
  ],
  "overall_acceptance": ["æ–‡ä»¶åˆ›å»ºæˆåŠŸ"]
}}
```"""

            # Reflector å“åº”
            reflector_response = Mock()
            reflector_response.content = """```json
{
  "passed": true,
  "summary": "ä»»åŠ¡æˆåŠŸå®Œæˆã€‚",
  "feedback": "",
  "achievements": ["åˆ›å»ºäº†æ–‡ä»¶"],
  "issues": [],
  "recommendation": "completed"
}
```"""

            mock_llm.ainvoke = AsyncMock(side_effect=[
                planner_response,
                reflector_response
            ])

            return mock_llm

        mock_llm = create_mock_llm()

        checkpointer = InMemorySaver()
        graph = create_full_swarm_graph(
            workspace_path=tmp_path,
            checkpointer=checkpointer,
            planner={"llm": mock_llm, "min_subtasks": 1},
            coder={"llm": mock_llm},
            reviewer={"llm": mock_llm},
            researcher={"llm": mock_llm},
            tool_runner={"require_approval": True},
            stop_condition={},
            reflector={"llm": mock_llm}
        )

        thread = {"configurable": {"thread_id": "test-modify"}}

        # æ‰§è¡Œåˆ°ä¸­æ–­ç‚¹
        state = create_initial_state("åˆ›å»ºæ–‡ä»¶")
        await graph.ainvoke(state, thread)

        # éªŒè¯ä¸­æ–­
        current_state = graph.get_state(thread)
        assert current_state.interrupts

        interrupt_obj = current_state.interrupts[0]
        interrupt_data = interrupt_obj.value
        assert interrupt_data["details"]["tool_args"]["path"] == f"{tmp_path}/original.txt"

        # ç”¨æˆ·ä¿®æ”¹å‚æ•°
        user_decision = {
            "action": "modify",
            "operation": "tool_execution",
            "timestamp": "2026-01-22T00:00:00Z",
            "modified_data": {
                "tool_args": {
                    "path": f"{tmp_path}/modified.txt",  # ä¿®æ”¹æ–‡ä»¶å
                    "content": "Modified Content"  # ä¿®æ”¹å†…å®¹
                }
            }
        }

        # æ¢å¤æ‰§è¡Œ
        final_state = await graph.ainvoke(Command(resume=user_decision), thread)

        # éªŒè¯ï¼šåº”è¯¥åˆ›å»ºä¿®æ”¹åçš„æ–‡ä»¶
        assert final_state["status"] == "completed"
        assert final_state["subtask_results"][0]["passed"] is True

        # éªŒè¯æ–‡ä»¶
        modified_file = tmp_path / "modified.txt"
        original_file = tmp_path / "original.txt"

        assert modified_file.exists(), "Modified file should exist"
        assert not original_file.exists(), "Original file should NOT exist"
        assert modified_file.read_text() == "Modified Content"


@pytest.mark.asyncio
class TestHITLSequentialInterrupts:
    """æµ‹è¯•è¿ç»­ä¸­æ–­åœºæ™¯"""

    async def test_approve_then_deny(self, tmp_path):
        """æµ‹è¯•å…ˆæ‰¹å‡†åæ‹’ç»çš„åœºæ™¯"""

        mock_llm = AsyncMock()

        # Planner å“åº”ï¼ˆä¸¤ä¸ªå·¥å…·ä»»åŠ¡ï¼‰
        planner_response = Mock()
        planner_response.content = f"""```json
{{
  "task": "åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶",
  "subtasks": [
    {{
      "id": "task-1",
      "type": "tool",
      "description": "åˆ›å»ºç¬¬ä¸€ä¸ªæ–‡ä»¶",
      "dependencies": [],
      "acceptance_criteria": ["æ–‡ä»¶åˆ›å»ºæˆåŠŸ"],
      "tool_name": "write_file",
      "tool_args": {{
        "path": "{tmp_path}/file1.txt",
        "content": "File 1"
      }}
    }},
    {{
      "id": "task-2",
      "type": "tool",
      "description": "åˆ›å»ºç¬¬äºŒä¸ªæ–‡ä»¶",
      "dependencies": [],
      "acceptance_criteria": ["æ–‡ä»¶åˆ›å»ºæˆåŠŸ"],
      "tool_name": "write_file",
      "tool_args": {{
        "path": "{tmp_path}/file2.txt",
        "content": "File 2"
      }}
    }}
  ],
  "overall_acceptance": ["æ‰€æœ‰æ–‡ä»¶åˆ›å»ºæˆåŠŸ"]
}}
```"""

        # Reflector å“åº”
        reflector_response = Mock()
        reflector_response.content = """```json
{
  "passed": false,
  "summary": "éƒ¨åˆ†ä»»åŠ¡å¤±è´¥ã€‚",
  "feedback": "",
  "achievements": ["åˆ›å»ºäº† file1.txt"],
  "issues": ["file2.txt åˆ›å»ºå¤±è´¥"],
  "recommendation": "completed"
}
```"""

        mock_llm.ainvoke = AsyncMock(side_effect=[
            planner_response,
            reflector_response
        ])

        checkpointer = InMemorySaver()
        graph = create_full_swarm_graph(
            workspace_path=tmp_path,
            checkpointer=checkpointer,
            planner={"llm": mock_llm, "min_subtasks": 1},
            coder={"llm": mock_llm},
            reviewer={"llm": mock_llm},
            researcher={"llm": mock_llm},
            tool_runner={"require_approval": True},
            stop_condition={},
            reflector={"llm": mock_llm}
        )

        thread = {"configurable": {"thread_id": "test-approve-deny"}}

        # æ‰§è¡Œåˆ°ç¬¬ä¸€ä¸ªä¸­æ–­ç‚¹
        state = create_initial_state("åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶")
        await graph.ainvoke(state, thread)

        # ç¬¬ä¸€æ¬¡ï¼šæ‰¹å‡†
        await graph.ainvoke(
            Command(resume={"action": "approve", "operation": "tool_execution", "timestamp": "2026-01-22T00:00:00Z"}),
            thread
        )

        # ç¬¬äºŒæ¬¡ï¼šæ‹’ç»
        final_state = await graph.ainvoke(
            Command(resume={"action": "deny", "operation": "tool_execution", "timestamp": "2026-01-22T00:00:01Z"}),
            thread
        )

        # éªŒè¯ç»“æœï¼ˆç”±äºæœ‰ä»»åŠ¡å¤±è´¥ï¼ŒReflector åº”è¯¥æ ‡è®°ä¸º failedï¼‰
        # Reflector Mock å“åº”è®¾ç½®ä¸º "passed": falseï¼Œæ‰€ä»¥æœ€ç»ˆçŠ¶æ€åº”è¯¥æ˜¯ failed
        # ä½†è¿™æ˜¯æ­£ç¡®çš„è¡Œä¸º - ç”¨æˆ·æ‹’ç»äº†ä¸€ä¸ªä»»åŠ¡
        assert final_state["status"] in ["completed", "failed"]  # â† ä¸¤ç§çŠ¶æ€éƒ½å¯ä»¥æ¥å—
        assert len(final_state["subtask_results"]) == 2

        # ç¬¬ä¸€ä¸ªä»»åŠ¡åº”è¯¥æˆåŠŸ
        assert final_state["subtask_results"][0]["passed"] is True
        assert (tmp_path / "file1.txt").exists()

        # ç¬¬äºŒä¸ªä»»åŠ¡åº”è¯¥å¤±è´¥ï¼ˆç”¨æˆ·æ‹’ç»ï¼‰
        assert final_state["subtask_results"][1]["passed"] is False
        assert "ç”¨æˆ·æ‹’ç»" in final_state["subtask_results"][1]["error_message"]
        assert not (tmp_path / "file2.txt").exists()


@pytest.mark.asyncio
class TestHITLWithoutCheckpointer:
    """æµ‹è¯•æ²¡æœ‰ checkpointer æ—¶çš„è¡Œä¸º"""

    async def test_hitl_requires_checkpointer(self, tmp_path):
        """æµ‹è¯• HITL éœ€è¦ checkpointer"""

        def create_mock_llm():
            mock_llm = AsyncMock()

            planner_response = Mock()
            planner_response.content = f"""```json
{{
  "task": "åˆ›å»ºæ–‡ä»¶",
  "subtasks": [
    {{
      "id": "task-1",
      "type": "tool",
      "description": "åˆ›å»ºæµ‹è¯•æ–‡ä»¶",
      "dependencies": [],
      "acceptance_criteria": ["æ–‡ä»¶åˆ›å»ºæˆåŠŸ"],
      "tool_name": "write_file",
      "tool_args": {{
        "path": "{tmp_path}/test.txt",
        "content": "Test"
      }}
    }}
  ],
  "overall_acceptance": ["æ–‡ä»¶åˆ›å»ºæˆåŠŸ"]
}}
```"""

            reflector_response = Mock()
            reflector_response.content = """```json
{
  "passed": true,
  "summary": "ä»»åŠ¡æˆåŠŸå®Œæˆã€‚",
  "feedback": "",
  "achievements": ["åˆ›å»ºäº†æ–‡ä»¶"],
  "issues": [],
  "recommendation": "completed"
}
```"""

            mock_llm.ainvoke = AsyncMock(side_effect=[
                planner_response,
                reflector_response
            ])

            return mock_llm

        mock_llm = create_mock_llm()

        # å°è¯•åˆ›å»º graph WITHOUT checkpointerï¼ˆåº”è¯¥æŠ›å‡º ValueErrorï¼‰
        with pytest.raises(ValueError) as exc_info:
            graph = create_full_swarm_graph(
                workspace_path=tmp_path,
                checkpointer=None,  # â† æ²¡æœ‰ checkpointer
                planner={"llm": mock_llm, "min_subtasks": 1},
                coder={"llm": mock_llm},
                reviewer={"llm": mock_llm},
                researcher={"llm": mock_llm},
                tool_runner={"require_approval": True},  # â† å¯ç”¨ HITL
                stop_condition={},
                reflector={"llm": mock_llm}
            )

        # éªŒè¯é”™è¯¯ä¿¡æ¯æ¸…æ™°
        error_message = str(exc_info.value)
        assert "Human-in-the-Loop requires checkpointer" in error_message
        assert "InMemorySaver" in error_message or "MemorySaver" in error_message


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
