#!/usr/bin/env python3
"""
MacCortex Pattern æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•æ‰€æœ‰ 5 ä¸ª Pattern çš„æ€§èƒ½æŒ‡æ ‡
"""

import asyncio
import httpx
import time
import statistics
from typing import List, Dict, Any

BASE_URL = "http://127.0.0.1:8000"

class PerformanceBenchmark:
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=60.0)
        self.results = {}

    async def benchmark_pattern(
        self,
        pattern_id: str,
        text: str,
        parameters: Dict[str, Any],
        iterations: int = 10
    ) -> Dict[str, Any]:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•å•ä¸ª Pattern"""
        latencies = []
        errors = 0

        print(f"\nğŸ”¬ æµ‹è¯• {pattern_id} ({iterations} æ¬¡è¿­ä»£)...")

        for i in range(iterations):
            request_data = {
                "request_id": f"perf-{pattern_id}-{i}",
                "pattern_id": pattern_id,
                "text": text,
                "parameters": parameters,
            }

            try:
                start_time = time.time()
                response = await self.client.post(f"{BASE_URL}/execute", json=request_data)
                response.raise_for_status()
                result = response.json()
                end_time = time.time()

                if result["success"]:
                    latency = end_time - start_time
                    latencies.append(latency)
                else:
                    errors += 1
            except Exception as e:
                errors += 1
                print(f"   âŒ è¿­ä»£ {i+1} å¤±è´¥: {str(e)}")

        if latencies:
            return {
                "pattern_id": pattern_id,
                "iterations": len(latencies),
                "errors": errors,
                "min_latency": min(latencies),
                "max_latency": max(latencies),
                "avg_latency": statistics.mean(latencies),
                "median_latency": statistics.median(latencies),
                "stdev_latency": statistics.stdev(latencies) if len(latencies) > 1 else 0,
            }
        else:
            return {
                "pattern_id": pattern_id,
                "errors": errors,
                "status": "failed",
            }

    async def benchmark_concurrent(
        self,
        requests: List[Dict[str, Any]],
        concurrency: int = 5
    ) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›"""
        print(f"\nâš¡ï¸ å¹¶å‘æµ‹è¯• ({concurrency} å¹¶å‘è¯·æ±‚)...")

        start_time = time.time()
        tasks = []

        for request_data in requests[:concurrency]:
            task = self.client.post(f"{BASE_URL}/execute", json=request_data)
            tasks.append(task)

        try:
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()

            successful = sum(1 for r in responses if isinstance(r, httpx.Response) and r.status_code == 200)
            failed = len(responses) - successful
            total_time = end_time - start_time

            return {
                "concurrency": concurrency,
                "total_requests": len(requests[:concurrency]),
                "successful": successful,
                "failed": failed,
                "total_time": total_time,
                "avg_time_per_request": total_time / concurrency,
            }
        except Exception as e:
            return {
                "concurrency": concurrency,
                "error": str(e),
                "status": "failed",
            }

    async def run_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n" + "="*70)
        print("MacCortex Pattern æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*70)

        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "pattern_id": "summarize",
                "text": "MacCortex æ˜¯ä¸‹ä¸€ä»£ macOS ä¸ªäººæ™ºèƒ½åŸºç¡€è®¾æ–½ï¼ŒåŸºäº Swarm Intelligence + Personal Infrastructure èåˆæ¶æ„ï¼Œæä¾›æœ¬åœ°ä¼˜å…ˆçš„ AI èƒ½åŠ›ï¼Œæ”¯æŒ Notes ç®¡ç†ã€æ™ºèƒ½æœç´¢ã€æ ¼å¼è½¬æ¢ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚é¡¹ç›®é‡‡ç”¨ Swift + Python æ··åˆæ¶æ„ï¼ŒSwift è´Ÿè´£ macOS åŸç”Ÿé›†æˆï¼ŒPython è´Ÿè´£ AI Pattern å¤„ç†ã€‚ç³»ç»Ÿé›†æˆ MLXï¼ˆApple Silicon ä¼˜åŒ–ï¼‰å’Œ Ollama æœ¬åœ° LLMï¼Œå®ç°ç«¯åˆ°ç«¯çš„æœ¬åœ°åŒ–æ™ºèƒ½å¤„ç†ã€‚",
                "parameters": {"length": "short", "style": "paragraph", "language": "zh-CN"},
            },
            {
                "pattern_id": "extract",
                "text": "å¼ ä¸‰æ˜¯è‹¹æœå…¬å¸çš„å·¥ç¨‹å¸ˆï¼Œä»–çš„é‚®ç®±æ˜¯ zhangsan@apple.comï¼Œç”µè¯æ˜¯ +86-138-0000-0000ã€‚ä»–åœ¨åŒ—äº¬å’Œä¸Šæµ·ä¸¤åœ°å·¥ä½œï¼Œé¡¹ç›®å¯åŠ¨æ—¥æœŸæ˜¯ 2026-01-20ã€‚",
                "parameters": {
                    "entity_types": ["person", "organization", "location"],
                    "extract_keywords": True,
                    "extract_contacts": True,
                    "extract_dates": True,
                },
            },
            {
                "pattern_id": "translate",
                "text": "MacCortex æ˜¯ä¸‹ä¸€ä»£ macOS ä¸ªäººæ™ºèƒ½åŸºç¡€è®¾æ–½",
                "parameters": {"target_language": "en", "style": "formal"},
            },
            {
                "pattern_id": "format",
                "text": '{"name": "MacCortex", "version": "0.1.0", "platform": "macOS"}',
                "parameters": {"from_format": "json", "to_format": "yaml", "prettify": True},
            },
            {
                "pattern_id": "search",
                "text": "macOS SwiftUI best practices",
                "parameters": {"search_type": "web", "engine": "duckduckgo", "num_results": 3},
            },
        ]

        # å•ä¸ª Pattern æ€§èƒ½æµ‹è¯•
        print("\nğŸ“Š å•ä¸ª Pattern æ€§èƒ½æµ‹è¯• (æ¯ä¸ª 10 æ¬¡è¿­ä»£)")
        print("-" * 70)

        for test_case in test_cases:
            result = await self.benchmark_pattern(
                test_case["pattern_id"],
                test_case["text"],
                test_case["parameters"],
                iterations=10
            )
            self.results[test_case["pattern_id"]] = result

            if "status" not in result or result.get("status") != "failed":
                print(f"   âœ… {result['pattern_id']}")
                print(f"      å¹³å‡å»¶è¿Ÿ: {result['avg_latency']*1000:.1f}ms")
                print(f"      ä¸­ä½å»¶è¿Ÿ: {result['median_latency']*1000:.1f}ms")
                print(f"      æœ€å°å»¶è¿Ÿ: {result['min_latency']*1000:.1f}ms")
                print(f"      æœ€å¤§å»¶è¿Ÿ: {result['max_latency']*1000:.1f}ms")
                print(f"      æ ‡å‡†å·®:   {result['stdev_latency']*1000:.1f}ms")
                if result['errors'] > 0:
                    print(f"      âš ï¸  é”™è¯¯: {result['errors']} æ¬¡")
            else:
                print(f"   âŒ {result['pattern_id']} - æµ‹è¯•å¤±è´¥")

        # å¹¶å‘æµ‹è¯•
        concurrent_requests = [
            {
                "request_id": f"concurrent-{i}",
                "pattern_id": test_cases[i % len(test_cases)]["pattern_id"],
                "text": test_cases[i % len(test_cases)]["text"],
                "parameters": test_cases[i % len(test_cases)]["parameters"],
            }
            for i in range(10)
        ]

        concurrent_result = await self.benchmark_concurrent(concurrent_requests, concurrency=5)
        self.results["concurrent"] = concurrent_result

        # æ€»ç»“æŠ¥å‘Š
        print("\n" + "="*70)
        print("æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("="*70)

        # è®¡ç®—æ€»ä½“å¹³å‡å»¶è¿Ÿ
        valid_results = [
            r for r in self.results.values()
            if isinstance(r, dict) and "avg_latency" in r
        ]

        if valid_results:
            overall_avg = statistics.mean([r["avg_latency"] for r in valid_results])
            overall_median = statistics.median([r["median_latency"] for r in valid_results])

            print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:")
            print(f"   å¹³å‡å»¶è¿Ÿ: {overall_avg*1000:.1f}ms")
            print(f"   ä¸­ä½å»¶è¿Ÿ: {overall_median*1000:.1f}ms")

            # æ€§èƒ½ç›®æ ‡æ£€æŸ¥
            target_latency = 2.5  # ç›®æ ‡: < 2.5s
            print(f"\nğŸ¯ æ€§èƒ½ç›®æ ‡: < {target_latency*1000:.0f}ms")

            passed = [r for r in valid_results if r["avg_latency"] < target_latency]
            print(f"   é€šè¿‡: {len(passed)}/{len(valid_results)} Patterns")

            if overall_avg < target_latency:
                print(f"   âœ… æ€»ä½“æ€§èƒ½è¾¾æ ‡ (å¹³å‡ {overall_avg*1000:.1f}ms < {target_latency*1000:.0f}ms)")
            else:
                print(f"   âš ï¸  æ€»ä½“æ€§èƒ½æœªè¾¾æ ‡ (å¹³å‡ {overall_avg*1000:.1f}ms > {target_latency*1000:.0f}ms)")

        # å¹¶å‘æµ‹è¯•ç»“æœ
        if "concurrent" in self.results:
            cr = self.results["concurrent"]
            if "error" not in cr:
                print(f"\nâš¡ï¸ å¹¶å‘æµ‹è¯•:")
                print(f"   å¹¶å‘æ•°: {cr['concurrency']}")
                print(f"   æˆåŠŸ: {cr['successful']}/{cr['total_requests']}")
                print(f"   æ€»è€—æ—¶: {cr['total_time']*1000:.1f}ms")
                print(f"   å¹³å‡æ¯è¯·æ±‚: {cr['avg_time_per_request']*1000:.1f}ms")

                if cr['failed'] == 0:
                    print(f"   âœ… å¹¶å‘æµ‹è¯•é€šè¿‡ (æ— å¤±è´¥)")
                else:
                    print(f"   âš ï¸  éƒ¨åˆ†è¯·æ±‚å¤±è´¥ ({cr['failed']} å¤±è´¥)")

        print("\n" + "="*70 + "\n")

        await self.client.aclose()

async def main():
    benchmark = PerformanceBenchmark()
    await benchmark.run_benchmarks()

if __name__ == "__main__":
    asyncio.run(main())
