#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) 2026 Yu Geng. All rights reserved.
# MacCortex - Proprietary and Confidential

"""
MacCortex Backend - Summarize Pattern
Phase 1 - Week 2 Day 8-9
åˆ›å»ºæ—¶é—´: 2026-01-20
æ›´æ–°æ—¶é—´: 2026-01-21 (Phase 1.5 - Day 3: é›†æˆ PromptGuard)

æ–‡æœ¬æ€»ç»“ Patternï¼ˆä½¿ç”¨ MLX æˆ– Ollamaï¼‰
Phase 1.5: å¢å¼ºå®‰å…¨é˜²æŠ¤ï¼ˆPrompt Injection æ£€æµ‹ã€æŒ‡ä»¤éš”ç¦»ã€è¾“å‡ºæ¸…ç†ï¼‰
"""

import asyncio
from typing import Any, Dict

from loguru import logger

from utils.config import settings
from patterns.base import BasePattern


class SummarizePattern(BasePattern):
    """æ–‡æœ¬æ€»ç»“ Pattern"""

    def __init__(self):
        """åˆå§‹åŒ– Pattern"""
        super().__init__()
        self._mlx_model = None
        self._ollama_client = None

    @property
    def pattern_id(self) -> str:
        return "summarize"

    @property
    def name(self) -> str:
        return "Summarize"

    @property
    def description(self) -> str:
        return "Summarize long text into concise key points"

    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        logger.info(f"ğŸ”§ åˆå§‹åŒ– {self.name} Pattern...")

        # å°è¯•åŠ è½½ MLX æ¨¡å‹ï¼ˆApple Silicon ä¼˜åŒ–ï¼‰
        try:
            await self._initialize_mlx()
        except Exception as e:
            logger.warning(f"MLX åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Ollama: {e}")
            try:
                await self._initialize_ollama()
            except Exception as e2:
                logger.warning(f"Ollama åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ Mock æ¨¡å¼: {e2}")
                logger.info("  âš ï¸  ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰")

    async def _initialize_mlx(self):
        """åˆå§‹åŒ– MLX æ¨¡å‹"""
        try:
            import mlx.core as mx
            from mlx_lm import load, generate

            logger.info(f"  ğŸ åŠ è½½ MLX æ¨¡å‹: {settings.mlx_model}")

            # å¼‚æ­¥åŠ è½½æ¨¡å‹ï¼ˆé¿å…é˜»å¡ï¼‰
            loop = asyncio.get_event_loop()
            self._mlx_model, self._mlx_tokenizer = await loop.run_in_executor(
                None, load, settings.mlx_model
            )

            logger.info("  âœ… MLX æ¨¡å‹åŠ è½½æˆåŠŸ")
        except ImportError:
            raise RuntimeError("MLX not installed. Install with: pip install mlx mlx-lm")
        except Exception as e:
            raise RuntimeError(f"Failed to load MLX model: {e}")

    async def _initialize_ollama(self):
        """åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯"""
        try:
            import ollama

            self._ollama_client = ollama.AsyncClient(host=settings.ollama_host)

            # æµ‹è¯•è¿æ¥
            await self._ollama_client.list()
            logger.info(f"  âœ… Ollama å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ ({settings.ollama_model})")
        except ImportError:
            raise RuntimeError("Ollama not installed. Install with: pip install ollama")
        except Exception as e:
            raise RuntimeError(f"Failed to connect to Ollama: {e}")

    def validate(self, text: str, parameters: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å…¥"""
        if not super().validate(text, parameters):
            return False

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼ˆè¯æ•°ï¼‰
        text = text.strip()
        language = parameters.get("language", "zh-CN")

        if language.startswith("zh") or language.startswith("ja") or language.startswith("ko"):
            # ä¸­æ—¥éŸ©ï¼šæ¯ä¸ªå­—ç¬¦çº¦ç­‰äºä¸€ä¸ªè¯
            word_count = len(text)
            min_words = 15
        else:
            # è¥¿æ–‡ï¼šæŒ‰ç©ºæ ¼åˆ†è¯
            word_count = len([w for w in text.split() if w])
            min_words = 30

        if word_count < min_words:
            logger.warning(
                f"æ–‡æœ¬è¿‡çŸ­: {word_count} è¯ < {min_words} è¯ï¼ˆè¯­è¨€: {language}ï¼‰"
            )
            return False

        return True

    async def execute(
        self, text: str, parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ‰§è¡Œæ€»ç»“ï¼ˆPhase 1.5: å¢å¼ºå®‰å…¨é˜²æŠ¤ï¼‰"""
        # æå–å‚æ•°
        length = parameters.get("length", "medium")
        style = parameters.get("style", "bullet")
        language = parameters.get("language", "zh-CN")
        source = parameters.get("source", "user")  # Phase 1.5: è¾“å…¥æ¥æº

        logger.info(
            f"ğŸ“ æ€»ç»“æ–‡æœ¬: length={length}, style={style}, language={language}, source={source}"
        )

        # ==================== Phase 1.5: Layer 3 - æ£€æµ‹ Prompt Injection ====================
        injection_result = self._check_injection(text, source=source)
        if injection_result["is_malicious"]:
            logger.warning(
                f"ğŸ”’ æ£€æµ‹åˆ°æ½œåœ¨ Prompt Injection: "
                f"ç½®ä¿¡åº¦={injection_result['confidence']:.2%}, "
                f"ä¸¥é‡ç¨‹åº¦={injection_result['severity']}"
            )
            # æ ‡è®°ä¸ºä¸å¯ä¿¡è¾“å…¥ï¼ˆç»§ç»­å¤„ç†ï¼Œä½†åŠ å¼ºé˜²æŠ¤ï¼‰

        # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆä¸å«ç”¨æˆ·è¾“å…¥ï¼‰
        system_prompt = self._build_system_prompt(length, style, language)

        # ==================== Phase 1.5: Layer 1+2 - ä¿æŠ¤æç¤ºè¯ ====================
        protected_prompt = self._protect_prompt(system_prompt, text, source=source)

        # ä½¿ç”¨ MLX æˆ– Ollama ç”Ÿæˆæ€»ç»“
        if self._mlx_model is not None:
            output = await self._generate_with_mlx(protected_prompt)
        elif self._ollama_client is not None:
            output = await self._generate_with_ollama(protected_prompt)
        else:
            # Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            logger.info("  âš ï¸  ä½¿ç”¨ Mock è¾“å‡ºï¼ˆMLX/Ollama æœªå®‰è£…ï¼‰")
            output = await self._generate_mock(text, length, style, language)

        # ==================== Phase 1.5: Layer 5 - æ¸…ç†è¾“å‡º ====================
        output = self._sanitize_output(output, text)

        return {
            "output": output,
            "metadata": {
                "length": length,
                "style": style,
                "language": language,
                "source": source,
                "original_length": len(text),
                "summary_length": len(output),
                # Phase 1.5: å®‰å…¨å…ƒæ•°æ®
                "security": {
                    "injection_detected": injection_result["is_malicious"],
                    "injection_confidence": injection_result["confidence"],
                    "injection_severity": injection_result["severity"],
                },
            },
        }

    def _build_system_prompt(self, length: str, style: str, language: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºï¼ˆPhase 1.5: ä¸å«ç”¨æˆ·è¾“å…¥ï¼‰"""
        # é•¿åº¦ç›®æ ‡
        length_target = {"short": 50, "medium": 150, "long": 300}.get(length, 150)

        # é£æ ¼æè¿°
        style_desc = {
            "bullet": "ä½¿ç”¨è¦ç‚¹åˆ—è¡¨å½¢å¼",
            "paragraph": "ä½¿ç”¨æ®µè½å½¢å¼",
            "headline": "ä½¿ç”¨ç®€çŸ­æ ‡é¢˜å½¢å¼",
        }.get(style, "ä½¿ç”¨è¦ç‚¹åˆ—è¡¨å½¢å¼")

        # è¯­è¨€æç¤º
        lang_prompt = {
            "zh-CN": "è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”",
            "zh-TW": "è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”",
            "en": "Please respond in English",
            "ja": "æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„",
            "ko": "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”",
        }.get(language, "è¯·ç”¨ä¸­æ–‡å›ç­”")

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ€»ç»“åŠ©æ‰‹ã€‚
è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚æ€»ç»“ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼š
- é£æ ¼ï¼š{style_desc}
- ç›®æ ‡é•¿åº¦ï¼šçº¦ {length_target} å­—
- è¯­è¨€ï¼š{lang_prompt}

é‡è¦è§„åˆ™ï¼š
1. ä»…æ€»ç»“ç”¨æˆ·æä¾›çš„æ–‡æœ¬å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–ä¿¡æ¯
2. ä¿æŒå®¢è§‚ä¸­ç«‹ï¼Œä¸è¦æ·»åŠ ä¸ªäººè§‚ç‚¹
3. ä¸“æ³¨äºæ ¸å¿ƒè¦ç‚¹å’Œå…³é”®ä¿¡æ¯
"""

        return system_prompt

    def _build_prompt(
        self, text: str, length: str, style: str, language: str
    ) -> str:
        """æ„å»º LLM æç¤ºè¯ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼ŒPhase 1 ä»£ç ä½¿ç”¨ï¼‰"""
        system = self._build_system_prompt(length, style, language)
        return f"{system}\n\nåŸæ–‡ï¼š\n{text}\n\næ€»ç»“ï¼š"

    async def _generate_with_mlx(self, prompt: str) -> str:
        """ä½¿ç”¨ MLX ç”Ÿæˆæ–‡æœ¬"""
        from mlx_lm import generate

        logger.debug("  ğŸ ä½¿ç”¨ MLX ç”Ÿæˆ...")

        try:
            loop = asyncio.get_event_loop()
            output = await loop.run_in_executor(
                None,
                generate,
                self._mlx_model,
                self._mlx_tokenizer,
                prompt,
                settings.mlx_max_tokens,
            )
            return output.strip()
        except Exception as e:
            logger.error(f"MLX ç”Ÿæˆå¤±è´¥: {e}")
            raise RuntimeError(f"MLX generation failed: {e}")

    async def _generate_with_ollama(self, prompt: str) -> str:
        """ä½¿ç”¨ Ollama ç”Ÿæˆæ–‡æœ¬"""
        logger.debug(f"  ğŸ¦™ ä½¿ç”¨ Ollama ç”Ÿæˆ (model={settings.ollama_model})...")

        try:
            response = await self._ollama_client.generate(
                model=settings.ollama_model,
                prompt=prompt,
                options={
                    "temperature": settings.mlx_temperature,
                    "num_predict": settings.mlx_max_tokens,
                },
            )
            return response["response"].strip()
        except Exception as e:
            logger.error(f"Ollama ç”Ÿæˆå¤±è´¥: {e}")
            raise RuntimeError(f"Ollama generation failed: {e}")

    async def _generate_mock(
        self, text: str, length: str, style: str, language: str
    ) -> str:
        """ä½¿ç”¨ Mock æ¨¡å¼ç”Ÿæˆæ€»ç»“ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        import asyncio

        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.1)

        word_count = {"short": 50, "medium": 150, "long": 300}.get(length, 150)

        if style == "bullet":
            return f"""â€¢ æ ¸å¿ƒè¦ç‚¹ 1ï¼š[Mock è¾“å‡ºï¼ŒMLX/Ollama æœªå®‰è£…]
â€¢ æ ¸å¿ƒè¦ç‚¹ 2ï¼šåŸæ–‡é•¿åº¦ {len(text)} å­—ç¬¦
â€¢ æ ¸å¿ƒè¦ç‚¹ 3ï¼šç›®æ ‡æ€»ç»“é•¿åº¦ {word_count} å­—

âš ï¸ è¿™æ˜¯æµ‹è¯•ç”¨ Mock è¾“å‡ºï¼Œè¯·å®‰è£… MLX æˆ– Ollama ä»¥ä½¿ç”¨çœŸå® LLMã€‚"""
        elif style == "paragraph":
            return f"[Mock è¾“å‡º] è¿™æ˜¯ä¸€æ®µæ€»ç»“æ€§çš„æ®µè½æ–‡æœ¬ã€‚åŸæ–‡é•¿åº¦ {len(text)} å­—ç¬¦ï¼Œç›®æ ‡æ€»ç»“é•¿åº¦ {word_count} å­—ã€‚è¯·å®‰è£… MLX æˆ– Ollama ä»¥ä½¿ç”¨çœŸå® LLMã€‚"
        else:  # headline
            return f"[Mock è¾“å‡º] æ ¸å¿ƒæ ‡é¢˜ï¼šå…³é”®ä¿¡æ¯æ‘˜è¦ï¼ˆ{word_count} å­—ç›®æ ‡ï¼‰"

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        logger.debug(f"  ğŸ§¹ {self.name} Pattern èµ„æºå·²æ¸…ç†")
