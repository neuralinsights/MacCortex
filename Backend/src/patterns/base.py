#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) 2026 Yu Geng. All rights reserved.
# MacCortex - Proprietary and Confidential

"""
MacCortex Backend - Base Pattern
Phase 1 - Week 2 Day 8-9
åˆ›å»ºæ—¶é—´: 2026-01-20
æ›´æ–°æ—¶é—´: 2026-01-21 (Phase 1.5 - Day 3: æ·»åŠ å®‰å…¨é’©å­)

Python Pattern åŸºç±»å®šä¹‰
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, Optional

from loguru import logger


class BasePattern(ABC):
    """AI Pattern åŸºç±»ï¼ˆPhase 1.5: å¢å¼ºå®‰å…¨é˜²æŠ¤ï¼‰"""

    def __init__(self, enable_security: bool = True):
        """
        åˆå§‹åŒ– Pattern

        Args:
            enable_security: æ˜¯å¦å¯ç”¨å®‰å…¨é˜²æŠ¤ï¼ˆé»˜è®¤ Trueï¼‰
        """
        self._enable_security = enable_security
        self._prompt_guard: Optional[Any] = None  # å»¶è¿ŸåŠ è½½

        # åˆå§‹åŒ– PromptGuardï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self._enable_security:
            self._init_security()

    @property
    @abstractmethod
    def pattern_id(self) -> str:
        """Pattern IDï¼ˆå”¯ä¸€æ ‡è¯†ç¬¦ï¼‰"""
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        """Pattern åç§°"""
        pass

    @property
    @abstractmethod
    def description(self) -> str:
        """Pattern æè¿°"""
        pass

    @property
    def version(self) -> str:
        """Pattern ç‰ˆæœ¬"""
        return "1.0.0"

    @abstractmethod
    async def execute(
        self, text: str, parameters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œ Pattern

        Args:
            text: è¾“å…¥æ–‡æœ¬
            parameters: å‚æ•°å­—å…¸

        Returns:
            Dict[str, Any]: æ‰§è¡Œç»“æœ
                {
                    "output": str,           # è¾“å‡ºæ–‡æœ¬
                    "metadata": Dict[str, Any] | None,  # å…ƒæ•°æ®
                }

        Raises:
            ValueError: å‚æ•°æ— æ•ˆ
            RuntimeError: æ‰§è¡Œå¤±è´¥
        """
        pass

    def validate(self, text: str, parameters: Dict[str, Any]) -> bool:
        """
        éªŒè¯è¾“å…¥

        Args:
            text: è¾“å…¥æ–‡æœ¬
            parameters: å‚æ•°å­—å…¸

        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        # åŸºæœ¬éªŒè¯
        if not text or not text.strip():
            return False

        return True

    async def initialize(self):
        """åˆå§‹åŒ–èµ„æºï¼ˆå¦‚åŠ è½½æ¨¡å‹ï¼‰"""
        pass

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        pass

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸è¡¨ç¤º"""
        return {
            "id": self.pattern_id,
            "name": self.name,
            "description": self.description,
            "version": self.version,
        }

    # ==================== Phase 1.5: å®‰å…¨é’©å­ ====================

    def _init_security(self):
        """åˆå§‹åŒ–å®‰å…¨æ¨¡å—ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        try:
            from security.prompt_guard import get_prompt_guard

            self._prompt_guard = get_prompt_guard()
            logger.debug(f"âœ“ {self.pattern_id}: å®‰å…¨æ¨¡å—å·²å¯ç”¨")
        except ImportError as e:
            logger.warning(
                f"âš ï¸ {self.pattern_id}: æ— æ³•å¯¼å…¥å®‰å…¨æ¨¡å—ï¼Œå®‰å…¨é˜²æŠ¤å·²ç¦ç”¨: {e}"
            )
            self._enable_security = False

    def _check_injection(self, text: str, source: str = "user") -> Dict[str, Any]:
        """
        æ£€æµ‹ Prompt Injection æ”»å‡»ï¼ˆPhase 1.5ï¼‰

        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬
            source: è¾“å…¥æ¥æºï¼ˆuser, file, webï¼‰

        Returns:
            æ£€æµ‹ç»“æœå­—å…¸ {
                "is_malicious": bool,
                "confidence": float,
                "severity": str,
                "details": str
            }
        """
        if not self._enable_security or self._prompt_guard is None:
            return {
                "is_malicious": False,
                "confidence": 0.0,
                "severity": "none",
                "details": "Security disabled",
            }

        result = self._prompt_guard.detect_injection(text)

        return {
            "is_malicious": result.is_malicious,
            "confidence": result.confidence,
            "severity": result.severity,
            "details": result.details,
        }

    def _protect_prompt(
        self, system_prompt: str, user_input: str, source: str = "user"
    ) -> str:
        """
        ä¿æŠ¤æç¤ºè¯ï¼ˆåº”ç”¨ Layer 1 + Layer 2 é˜²æŠ¤ï¼‰

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_input: ç”¨æˆ·è¾“å…¥
            source: è¾“å…¥æ¥æº

        Returns:
            ä¿æŠ¤åçš„å®Œæ•´æç¤ºè¯
        """
        if not self._enable_security or self._prompt_guard is None:
            return f"{system_prompt}\n\n{user_input}"

        # Layer 1: æ ‡è®°ä¸å¯ä¿¡è¾“å…¥
        marked = self._prompt_guard.mark_untrusted(user_input, source=source)

        # Layer 2: æŒ‡ä»¤éš”ç¦»
        protected = self._prompt_guard.isolate_instructions(
            system_prompt, marked, already_marked=True
        )

        return protected

    def _sanitize_output(self, output: str, original_input: str = "") -> str:
        """
        æ¸…ç† LLM è¾“å‡ºï¼ˆLayer 5ï¼‰

        Args:
            output: LLM è¾“å‡ºæ–‡æœ¬
            original_input: åŸå§‹ç”¨æˆ·è¾“å…¥

        Returns:
            æ¸…ç†åçš„è¾“å‡º
        """
        if not self._enable_security or self._prompt_guard is None:
            return output

        cleaned, warnings = self._prompt_guard.sanitize_output(output, original_input)

        if warnings:
            logger.warning(
                f"ğŸ”’ {self.pattern_id}: è¾“å‡ºå·²æ¸…ç† ({len(warnings)} ä¸ªè­¦å‘Š)"
            )
            for warning in warnings:
                logger.debug(f"   - {warning}")

        return cleaned
