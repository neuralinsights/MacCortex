#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) 2026 Yu Geng. All rights reserved.
# MacCortex - Proprietary and Confidential

# MacCortex ExtractPattern - ä¿¡æ¯æå–æ¨¡å¼
# Phase 1 - Week 2 Day 9
# åˆ›å»ºæ—¶é—´: 2026-01-20
#
# ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå®ä½“ã€å…³é”®è¯ã€è”ç³»æ–¹å¼ã€æ—¥æœŸç­‰ï¼‰

import asyncio
from typing import Any, Dict
from loguru import logger

from .base import BasePattern
from utils.config import settings


class ExtractPattern(BasePattern):
    """
    ä¿¡æ¯æå– Pattern

    ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼š
    - å®ä½“è¯†åˆ«ï¼ˆäººåã€åœ°åã€ç»„ç»‡æœºæ„ï¼‰
    - å…³é”®è¯æå–
    - è”ç³»æ–¹å¼ï¼ˆé‚®ç®±ã€ç”µè¯ã€ç½‘å€ï¼‰
    - æ—¥æœŸæ—¶é—´
    - è‡ªå®šä¹‰å®ä½“
    """

    def __init__(self):
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        self._mode = "uninitialized"  # uninitialized | mlx | ollama | mock

    # MARK: - BasePattern Protocol

    @property
    def pattern_id(self) -> str:
        return "extract"

    @property
    def name(self) -> str:
        return "Extract"

    @property
    def description(self) -> str:
        return "ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå®ä½“ã€å…³é”®è¯ã€è”ç³»æ–¹å¼ç­‰ï¼‰"

    @property
    def version(self) -> str:
        return "1.0.0"

    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        logger.info(f"ğŸ”§ åˆå§‹åŒ– {self.name} Pattern...")

        # å°è¯•åŠ è½½ MLX æ¨¡å‹ï¼ˆApple Silicon ä¼˜åŒ–ï¼‰
        try:
            await self._initialize_mlx()
        except Exception as e:
            logger.warning(f"MLX åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Ollama: {e}")
            try:
                await self._initialize_ollama()
            except Exception as e2:
                logger.warning(f"Ollama åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ Mock æ¨¡å¼: {e2}")
                logger.info("  âš ï¸  ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰")

    async def _initialize_mlx(self):
        """åˆå§‹åŒ– MLX æ¨¡å‹"""
        try:
            import mlx.core as mx
            from mlx_lm import load

            logger.info(f"  ğŸ åŠ è½½ MLX æ¨¡å‹: {settings.mlx_model}")

            # å¼‚æ­¥åŠ è½½æ¨¡å‹ï¼ˆå¤ç”¨ SummarizePattern çš„æ¨¡å‹ï¼‰
            loop = asyncio.get_event_loop()
            self._mlx_model, self._mlx_tokenizer = await loop.run_in_executor(
                None, load, settings.mlx_model
            )

            self._mode = "mlx"
            logger.info("  âœ… MLX æ¨¡å‹åŠ è½½æˆåŠŸ")
        except ImportError:
            raise RuntimeError("MLX æœªå®‰è£…")
        except Exception as e:
            raise RuntimeError(f"MLX åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _initialize_ollama(self):
        """åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯"""
        try:
            import ollama

            logger.info(f"  ğŸ¦™ è¿æ¥ Ollama: {settings.ollama_model}")

            # æµ‹è¯•è¿æ¥
            client = ollama.AsyncClient()
            try:
                await client.generate(
                    model=settings.ollama_model, prompt="test", options={"num_predict": 1}
                )
                self._ollama_client = client
                self._mode = "ollama"
                logger.info("  âœ… Ollama è¿æ¥æˆåŠŸ")
            except Exception as e:
                raise RuntimeError(f"Ollama è¿æ¥å¤±è´¥: {e}")
        except ImportError:
            raise RuntimeError("Ollama æœªå®‰è£…")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        logger.info(f"âœ… {self.name} Pattern æ¸…ç†å®Œæˆ")

    async def execute(self, text: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¿¡æ¯æå–

        Args:
            text: è¾“å…¥æ–‡æœ¬
            parameters: æå–å‚æ•°
                - entity_types: å®ä½“ç±»å‹åˆ—è¡¨ (é»˜è®¤: ["person", "organization", "location"])
                - extract_keywords: æ˜¯å¦æå–å…³é”®è¯ (é»˜è®¤: true)
                - extract_contacts: æ˜¯å¦æå–è”ç³»æ–¹å¼ (é»˜è®¤: true)
                - extract_dates: æ˜¯å¦æå–æ—¥æœŸæ—¶é—´ (é»˜è®¤: true)
                - custom_entities: è‡ªå®šä¹‰å®ä½“ç±»å‹åˆ—è¡¨ (å¯é€‰)
                - language: è¯­è¨€ (é»˜è®¤: "zh-CN")

        Returns:
            æå–ç»“æœå­—å…¸
        """
        # è§£æå‚æ•°
        entity_types = parameters.get("entity_types", ["person", "organization", "location"])
        extract_keywords = parameters.get("extract_keywords", True)
        extract_contacts = parameters.get("extract_contacts", True)
        extract_dates = parameters.get("extract_dates", True)
        custom_entities = parameters.get("custom_entities", [])
        language = parameters.get("language", "zh-CN")

        # æ ¹æ®æ¨¡å¼é€‰æ‹©ç”Ÿæˆæ–¹æ³•
        if self._mode == "mlx":
            result = await self._extract_with_mlx(
                text, entity_types, extract_keywords, extract_contacts, extract_dates, custom_entities, language
            )
        elif self._mode == "ollama":
            result = await self._extract_with_ollama(
                text, entity_types, extract_keywords, extract_contacts, extract_dates, custom_entities, language
            )
        else:
            # Mock æ¨¡å¼
            result = await self._extract_mock(
                text, entity_types, extract_keywords, extract_contacts, extract_dates, custom_entities, language
            )

        # æ„å»ºæå–ç»“æœ
        extraction_result = {
            "entities": result.get("entities", {}),
            "keywords": result.get("keywords", []) if extract_keywords else [],
            "contacts": result.get("contacts", {}) if extract_contacts else {},
            "dates": result.get("dates", []) if extract_dates else [],
        }

        # åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼ˆç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼‰
        import json
        output = json.dumps(extraction_result, ensure_ascii=False, indent=2)

        return {
            "output": output,
            "metadata": {
                "entity_types": entity_types,
                "extract_keywords": extract_keywords,
                "extract_contacts": extract_contacts,
                "extract_dates": extract_dates,
                "custom_entities": custom_entities,
                "language": language,
                "text_length": len(text),
                "mode": self._mode,
            },
        }

    async def _extract_with_mlx(
        self,
        text: str,
        entity_types: list,
        extract_keywords: bool,
        extract_contacts: bool,
        extract_dates: bool,
        custom_entities: list,
        language: str,
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ MLX æ¨¡å‹è¿›è¡Œä¿¡æ¯æå–"""
        from mlx_lm import generate

        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(
            text, entity_types, extract_keywords, extract_contacts, extract_dates, custom_entities, language
        )

        # ç”Ÿæˆï¼ˆåŒæ­¥æ–¹æ³•ï¼Œéœ€è¦åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰
        loop = asyncio.get_event_loop()
        output = await loop.run_in_executor(
            None,
            generate,
            self._mlx_model,
            self._mlx_tokenizer,
            prompt,
            512,  # max_tokens
        )

        # è§£æè¾“å‡ºä¸ºç»“æ„åŒ–æ•°æ®
        return self._parse_extraction_output(output)

    async def _extract_with_ollama(
        self,
        text: str,
        entity_types: list,
        extract_keywords: bool,
        extract_contacts: bool,
        extract_dates: bool,
        custom_entities: list,
        language: str,
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ Ollama è¿›è¡Œä¿¡æ¯æå–"""
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(
            text, entity_types, extract_keywords, extract_contacts, extract_dates, custom_entities, language
        )

        # ç”Ÿæˆ
        response = await self._ollama_client.generate(
            model=settings.ollama_model, prompt=prompt, options={"temperature": 0.3, "num_predict": 512}
        )

        # è§£æè¾“å‡º
        return self._parse_extraction_output(response["response"])

    async def _extract_mock(
        self,
        text: str,
        entity_types: list,
        extract_keywords: bool,
        extract_contacts: bool,
        extract_dates: bool,
        custom_entities: list,
        language: str,
    ) -> Dict[str, Any]:
        """Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await asyncio.sleep(0.1)

        result = {"entities": {}, "keywords": [], "contacts": {}, "dates": []}

        # Mock å®ä½“æå–
        if "person" in entity_types:
            result["entities"]["person"] = ["å¼ ä¸‰", "æå››"] if language == "zh-CN" else ["John Doe", "Jane Smith"]
        if "organization" in entity_types:
            result["entities"]["organization"] = ["è‹¹æœå…¬å¸"] if language == "zh-CN" else ["Apple Inc."]
        if "location" in entity_types:
            result["entities"]["location"] = ["åŒ—äº¬", "ä¸Šæµ·"] if language == "zh-CN" else ["Beijing", "Shanghai"]

        # Mock å…³é”®è¯æå–
        if extract_keywords:
            result["keywords"] = (
                ["MacCortex", "æ™ºèƒ½", "åŸºç¡€è®¾æ–½"] if language == "zh-CN" else ["MacCortex", "intelligence", "infrastructure"]
            )

        # Mock è”ç³»æ–¹å¼æå–
        if extract_contacts:
            result["contacts"] = {"email": ["test@example.com"], "phone": ["+86-138-0000-0000"], "url": ["https://example.com"]}

        # Mock æ—¥æœŸæå–
        if extract_dates:
            result["dates"] = ["2026-01-20"]

        return result

    def _build_prompt(
        self,
        text: str,
        entity_types: list,
        extract_keywords: bool,
        extract_contacts: bool,
        extract_dates: bool,
        custom_entities: list,
        language: str,
    ) -> str:
        """æ„å»ºä¿¡æ¯æå–æç¤ºè¯"""
        if language == "zh-CN":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼š

æ–‡æœ¬ï¼š
{text}

æå–ä»»åŠ¡ï¼š
"""
            if entity_types:
                entity_names = {"person": "äººå", "organization": "ç»„ç»‡æœºæ„", "location": "åœ°ç‚¹"}
                types_str = "ã€".join([entity_names.get(t, t) for t in entity_types])
                prompt += f"1. å®ä½“è¯†åˆ«ï¼š{types_str}\n"

            if extract_keywords:
                prompt += "2. å…³é”®è¯æå–ï¼šæå– 3-5 ä¸ªæ ¸å¿ƒå…³é”®è¯\n"

            if extract_contacts:
                prompt += "3. è”ç³»æ–¹å¼ï¼šé‚®ç®±ã€ç”µè¯ã€ç½‘å€\n"

            if extract_dates:
                prompt += "4. æ—¥æœŸæ—¶é—´ï¼šæå–æ‰€æœ‰æ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯\n"

            if custom_entities:
                prompt += f"5. è‡ªå®šä¹‰å®ä½“ï¼š{', '.join(custom_entities)}\n"

            prompt += "\nè¯·ä»¥ JSON æ ¼å¼è¾“å‡ºç»“æœã€‚"
        else:
            prompt = f"""You are a professional information extraction assistant. Extract information from the following text:

Text:
{text}

Extraction tasks:
"""
            if entity_types:
                prompt += f"1. Named entities: {', '.join(entity_types)}\n"

            if extract_keywords:
                prompt += "2. Keywords: Extract 3-5 core keywords\n"

            if extract_contacts:
                prompt += "3. Contact information: emails, phones, URLs\n"

            if extract_dates:
                prompt += "4. Dates and times: Extract all date/time information\n"

            if custom_entities:
                prompt += f"5. Custom entities: {', '.join(custom_entities)}\n"

            prompt += "\nPlease output the result in JSON format."

        return prompt

    def _parse_extraction_output(self, output: str) -> Dict[str, Any]:
        """è§£ææå–è¾“å‡ºä¸ºç»“æ„åŒ–æ•°æ®"""
        import json
        import re

        # å°è¯•ç›´æ¥è§£æ JSON
        try:
            # æŸ¥æ‰¾ JSON ä»£ç å—
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", output, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))

            # æŸ¥æ‰¾è£¸ JSON
            json_match = re.search(r"\{.*\}", output, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
        except json.JSONDecodeError:
            pass

        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é»˜è®¤ç»“æ„
        logger.warning(f"æ— æ³•è§£ææå–è¾“å‡ºï¼Œè¿”å›é»˜è®¤ç»“æ„: {output[:100]}")
        return {"entities": {}, "keywords": [], "contacts": {}, "dates": []}
