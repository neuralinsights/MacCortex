#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) 2026 Yu Geng. All rights reserved.
# MacCortex - Proprietary and Confidential

# MacCortex SearchPattern - æœç´¢æ¨¡å¼
# Phase 1 - Week 2 Day 9
# åˆ›å»ºæ—¶é—´: 2026-01-20
# æ›´æ–°æ—¶é—´: 2026-01-21 (Phase 1.5 - Day 3: é›†æˆ PromptGuard)
#
# Phase 1.5: å¢å¼ºå®‰å…¨é˜²æŠ¤ï¼ˆPrompt Injection æ£€æµ‹ã€æŒ‡ä»¤éš”ç¦»ã€è¾“å‡ºæ¸…ç†ï¼‰
#
# Web æœç´¢ + è¯­ä¹‰æœç´¢ï¼ˆæœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢ï¼‰

import asyncio
import hashlib
import time
from typing import Any, Dict, List, Optional
from loguru import logger

from .base import BasePattern
from utils.config import settings


class SearchPattern(BasePattern):
    """
    æœç´¢ Pattern

    æ”¯æŒå¤šç§æœç´¢æ–¹å¼ï¼š
    - Web æœç´¢ï¼ˆGoogleã€DuckDuckGoã€Bingï¼‰
    - è¯­ä¹‰æœç´¢ï¼ˆæœ¬åœ°å‘é‡æ•°æ®åº“ï¼‰
    - æ··åˆæœç´¢ï¼ˆWeb + æœ¬åœ°ï¼‰
    - ç»“æœæ’åºä¸è¿‡æ»¤
    """

    def __init__(self):
        super().__init__()  # Phase 1.5: åˆå§‹åŒ–å®‰å…¨æ¨¡å—
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        self._vector_db = None  # ChromaDB å®¢æˆ·ç«¯
        self._mode = "uninitialized"  # uninitialized | mlx | ollama | mock

        # ç¼“å­˜æœºåˆ¶ï¼ˆPhase 2 Week 4 Day 17ï¼‰
        self._search_cache: Dict[str, tuple[List[Dict[str, Any]], float]] = {}
        self._cache_ttl = 300  # 5 åˆ†é’Ÿ TTL

    # MARK: - BasePattern Protocol

    @property
    def pattern_id(self) -> str:
        return "search"

    @property
    def name(self) -> str:
        return "Search"

    @property
    def description(self) -> str:
        return "Web æœç´¢ + è¯­ä¹‰æœç´¢ï¼ˆæœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢ï¼‰"

    @property
    def version(self) -> str:
        return "1.0.0"

    async def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹ä¸æœç´¢å®¢æˆ·ç«¯"""
        logger.info(f"ğŸ”§ åˆå§‹åŒ– {self.name} Pattern...")

        # åˆå§‹åŒ– LLMï¼ˆç”¨äºæŸ¥è¯¢é‡å†™ä¸ç»“æœæ€»ç»“ï¼‰
        try:
            await self._initialize_mlx()
        except Exception as e:
            logger.warning(f"MLX åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Ollama: {e}")
            try:
                await self._initialize_ollama()
            except Exception as e2:
                logger.warning(f"Ollama åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ Mock æ¨¡å¼: {e2}")
                logger.info("  âš ï¸  ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰")

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰
        try:
            await self._initialize_vector_db()
        except Exception as e:
            logger.warning(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _initialize_mlx(self):
        """åˆå§‹åŒ– MLX æ¨¡å‹"""
        try:
            import mlx.core as mx
            from mlx_lm import load

            logger.info(f"  ğŸ åŠ è½½ MLX æ¨¡å‹: {settings.mlx_model}")

            # å¼‚æ­¥åŠ è½½æ¨¡å‹ï¼ˆå¤ç”¨ SummarizePattern çš„æ¨¡å‹ï¼‰
            loop = asyncio.get_event_loop()
            self._mlx_model, self._mlx_tokenizer = await loop.run_in_executor(
                None, load, settings.mlx_model
            )

            self._mode = "mlx"
            logger.info("  âœ… MLX æ¨¡å‹åŠ è½½æˆåŠŸ")
        except ImportError:
            raise RuntimeError("MLX æœªå®‰è£…")
        except Exception as e:
            raise RuntimeError(f"MLX åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _initialize_ollama(self):
        """åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯"""
        try:
            import ollama

            logger.info(f"  ğŸ¦™ è¿æ¥ Ollama: {settings.ollama_model}")

            # æµ‹è¯•è¿æ¥
            client = ollama.AsyncClient()
            try:
                await client.generate(
                    model=settings.ollama_model, prompt="test", options={"num_predict": 1}
                )
                self._ollama_client = client
                self._mode = "ollama"
                logger.info("  âœ… Ollama è¿æ¥æˆåŠŸ")
            except Exception as e:
                raise RuntimeError(f"Ollama è¿æ¥å¤±è´¥: {e}")
        except ImportError:
            raise RuntimeError("Ollama æœªå®‰è£…")

    async def _initialize_vector_db(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆChromaDBï¼‰"""
        try:
            import chromadb

            logger.info("  ğŸ—„ï¸  è¿æ¥ ChromaDB...")
            self._vector_db = chromadb.Client()
            logger.info("  âœ… ChromaDB è¿æ¥æˆåŠŸ")
        except ImportError:
            logger.warning("ChromaDB æœªå®‰è£…ï¼Œè¯­ä¹‰æœç´¢åŠŸèƒ½ä¸å¯ç”¨")
        except Exception as e:
            logger.warning(f"ChromaDB åˆå§‹åŒ–å¤±è´¥: {e}")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        self._vector_db = None
        logger.info(f"âœ… {self.name} Pattern æ¸…ç†å®Œæˆ")

    async def execute(self, text: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæœç´¢

        Args:
            text: æœç´¢æŸ¥è¯¢
            parameters: æœç´¢å‚æ•°
                - search_type: æœç´¢ç±»å‹ ("web" | "semantic" | "hybrid", é»˜è®¤ "web")
                - engine: æœç´¢å¼•æ“ ("google" | "duckduckgo" | "bing", é»˜è®¤ "duckduckgo")
                - num_results: è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 5)
                - summarize: æ˜¯å¦æ€»ç»“æœç´¢ç»“æœ (é»˜è®¤: true)
                - language: æœç´¢è¯­è¨€ (é»˜è®¤: "zh-CN")
                - collection: è¯­ä¹‰æœç´¢çš„é›†åˆåç§° (é»˜è®¤: "default")

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        # è§£æå‚æ•°
        search_type = parameters.get("search_type", "web")
        engine = parameters.get("engine", "duckduckgo")
        num_results = parameters.get("num_results", 5)
        summarize = parameters.get("summarize", True)
        language = parameters.get("language", "zh-CN")
        collection = parameters.get("collection", "default")

        # æ‰§è¡Œæœç´¢
        if search_type == "web":
            results = await self._web_search(text, engine, num_results, language)
        elif search_type == "semantic":
            results = await self._semantic_search(text, collection, num_results)
        elif search_type == "hybrid":
            web_results = await self._web_search(text, engine, num_results // 2, language)
            semantic_results = await self._semantic_search(text, collection, num_results // 2)
            results = web_results + semantic_results
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")

        # æ€»ç»“æœç´¢ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
        summary = None
        if summarize and results:
            summary = await self._summarize_results(text, results)

        # åºåˆ—åŒ–æœç´¢ç»“æœä¸º JSON å­—ç¬¦ä¸²ï¼ˆç»Ÿä¸€è¾“å‡ºæ ¼å¼ï¼‰
        search_result = {
            "results": results[:num_results],
            "summary": summary,
        }

        import json
        output = json.dumps(search_result, ensure_ascii=False, indent=2)

        return {
            "output": output,  # ç»Ÿä¸€è¾“å‡ºæ ¼å¼
            "metadata": {
                "search_type": search_type,
                "engine": engine,
                "num_results": num_results,
                "query": text,
                "total_found": len(results),
                "mode": self._mode,
            },
        }

    async def _web_search(self, query: str, engine: str, num_results: int, language: str) -> List[Dict[str, Any]]:
        """Web æœç´¢"""
        if engine == "duckduckgo":
            return await self._search_duckduckgo(query, num_results, language)
        elif engine == "google":
            return await self._search_google(query, num_results, language)
        elif engine == "bing":
            return await self._search_bing(query, num_results, language)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {engine}")

    async def _search_duckduckgo(self, query: str, num_results: int, language: str) -> List[Dict[str, Any]]:
        """
        DuckDuckGo æœç´¢ï¼ˆPhase 2 Week 4 Day 17 ä¼˜åŒ–ï¼‰

        æ”¹è¿›ï¼š
        1. 5 åˆ†é’Ÿç¼“å­˜ï¼ˆå‡å°‘ API è°ƒç”¨ï¼‰
        2. æ›´å¥½çš„é”™è¯¯å¤„ç†ï¼ˆè¶…æ—¶ã€é€Ÿç‡é™åˆ¶ï¼‰
        3. è¯­è¨€æ˜ å°„æ‰©å±•ï¼ˆæ”¯æŒæ›´å¤šè¯­è¨€ï¼‰
        4. æ—¥å¿—è®°å½•ä¼˜åŒ–
        """
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(query, num_results, language)
        cached_result = self._get_from_cache(cache_key)
        if cached_result is not None:
            logger.debug(f"ğŸš€ ä½¿ç”¨ç¼“å­˜ç»“æœ: {query} ({len(cached_result)} æ¡)")
            return cached_result

        # 2. æ‰§è¡Œæœç´¢
        try:
            from duckduckgo_search import DDGS

            # è¯­è¨€/åŒºåŸŸæ˜ å°„ï¼ˆPhase 2 Week 4 Day 17 æ‰©å±•ï¼‰
            region_map = {
                "zh-CN": "cn-zh",  # ä¸­å›½ç®€ä½“
                "zh": "cn-zh",
                "en-US": "us-en",  # ç¾å›½è‹±æ–‡
                "en": "us-en",
                "ja-JP": "jp-jp",  # æ—¥æœ¬
                "ja": "jp-jp",
                "ko-KR": "kr-kr",  # éŸ©å›½
                "ko": "kr-kr",
                "auto": "wt-wt",    # å…¨çƒï¼ˆæ— åœ°åŒºé™åˆ¶ï¼‰
            }
            region = region_map.get(language, "wt-wt")

            logger.info(f"ğŸ” DuckDuckGo æœç´¢: '{query}' (region={region}, num={num_results})")

            # æ‰§è¡Œæœç´¢ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œéœ€è¦åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œä»¥é¿å…é˜»å¡ï¼‰
            loop = asyncio.get_event_loop()
            results = []

            def _sync_search():
                """åŒæ­¥æœç´¢å‡½æ•°ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
                nonlocal results
                try:
                    with DDGS() as ddgs:
                        search_results = ddgs.text(
                            keywords=query,
                            region=region,
                            max_results=num_results * 2,  # å¤šè·å–ä¸€äº›ä»¥é˜²è¿‡æ»¤åä¸å¤Ÿ
                        )
                        for i, result in enumerate(search_results):
                            # è¿‡æ»¤æ— æ•ˆç»“æœ
                            if not result.get("title") or not result.get("href"):
                                continue

                            results.append(
                                {
                                    "title": result.get("title", ""),
                                    "url": result.get("href", ""),
                                    "snippet": result.get("body", ""),
                                    "source": "duckduckgo",
                                    "rank": i + 1,
                                }
                            )

                            # é™åˆ¶ç»“æœæ•°é‡
                            if len(results) >= num_results:
                                break
                except Exception as e:
                    logger.error(f"DuckDuckGo æœç´¢å†…éƒ¨é”™è¯¯: {e}")
                    raise

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
            await loop.run_in_executor(None, _sync_search)

            if not results:
                logger.warning(f"DuckDuckGo æœç´¢æ— ç»“æœ: '{query}'")
                return []

            logger.info(f"âœ… DuckDuckGo æœç´¢æˆåŠŸ: {len(results)} æ¡ç»“æœ")

            # 3. ç¼“å­˜ç»“æœ
            self._save_to_cache(cache_key, results)

            return results

        except ImportError:
            logger.error("duckduckgo_search æœªå®‰è£…ï¼ˆä½†å·²åœ¨ requirements.txt ä¸­ï¼‰")
            logger.info("  âš ï¸  å›é€€åˆ° Mock æœç´¢")
            return await self._mock_web_search(query, num_results)
        except Exception as e:
            logger.error(f"DuckDuckGo æœç´¢å¤±è´¥: {type(e).__name__}: {e}")
            logger.info("  âš ï¸  å›é€€åˆ° Mock æœç´¢")
            return await self._mock_web_search(query, num_results)

    async def _search_google(self, query: str, num_results: int, language: str) -> List[Dict[str, Any]]:
        """Google æœç´¢ï¼ˆéœ€è¦ API Keyï¼‰"""
        # TODO: å®ç° Google Custom Search API
        logger.warning("Google æœç´¢æš‚æœªå®ç°ï¼Œä½¿ç”¨ Mock æœç´¢")
        return await self._mock_web_search(query, num_results)

    async def _search_bing(self, query: str, num_results: int, language: str) -> List[Dict[str, Any]]:
        """Bing æœç´¢ï¼ˆéœ€è¦ API Keyï¼‰"""
        # TODO: å®ç° Bing Search API
        logger.warning("Bing æœç´¢æš‚æœªå®ç°ï¼Œä½¿ç”¨ Mock æœç´¢")
        return await self._mock_web_search(query, num_results)

    async def _mock_web_search(self, query: str, num_results: int) -> List[Dict[str, Any]]:
        """Mock Web æœç´¢ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await asyncio.sleep(0.1)

        return [
            {
                "title": f"MacCortex æœç´¢ç»“æœ {i + 1}",
                "url": f"https://example.com/result{i + 1}",
                "snippet": f"è¿™æ˜¯å…³äº '{query}' çš„æœç´¢ç»“æœ {i + 1}ã€‚MacCortex æ˜¯ä¸‹ä¸€ä»£ macOS ä¸ªäººæ™ºèƒ½åŸºç¡€è®¾æ–½ã€‚",
                "source": "mock",
                "rank": i + 1,
            }
            for i in range(num_results)
        ]

    async def _semantic_search(self, query: str, collection: str, num_results: int) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢ï¼ˆå‘é‡æ•°æ®åº“ï¼‰"""
        if not self._vector_db:
            logger.warning("ChromaDB æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ Mock è¯­ä¹‰æœç´¢")
            return await self._mock_semantic_search(query, num_results)

        try:
            # è·å–é›†åˆ
            col = self._vector_db.get_or_create_collection(name=collection)

            # æŸ¥è¯¢
            results = col.query(query_texts=[query], n_results=num_results)

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results["documents"] and results["documents"][0]:
                for i, (doc, metadata, distance) in enumerate(
                    zip(results["documents"][0], results["metadatas"][0], results["distances"][0])
                ):
                    formatted_results.append(
                        {
                            "title": metadata.get("title", f"æ–‡æ¡£ {i + 1}"),
                            "content": doc,
                            "metadata": metadata,
                            "similarity": 1.0 - distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                            "source": "chromadb",
                            "rank": i + 1,
                        }
                    )

            return formatted_results
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return await self._mock_semantic_search(query, num_results)

    async def _mock_semantic_search(self, query: str, num_results: int) -> List[Dict[str, Any]]:
        """Mock è¯­ä¹‰æœç´¢ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await asyncio.sleep(0.1)

        return [
            {
                "title": f"æœ¬åœ°æ–‡æ¡£ {i + 1}",
                "content": f"è¿™æ˜¯å…³äº '{query}' çš„æœ¬åœ°æ–‡æ¡£ {i + 1}ã€‚åŒ…å«ç›¸å…³ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ã€‚",
                "metadata": {"source": "mock", "created_at": "2026-01-20"},
                "similarity": 0.9 - (i * 0.1),
                "source": "mock_vector_db",
                "rank": i + 1,
            }
            for i in range(num_results)
        ]

    async def _summarize_results(self, query: str, results: List[Dict[str, Any]]) -> str:
        """æ€»ç»“æœç´¢ç»“æœ"""
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚"

        # æ„å»ºæ€»ç»“æç¤ºè¯
        results_text = "\n\n".join(
            [f"ç»“æœ {i + 1}: {r.get('title', '')}\n{r.get('snippet', '') or r.get('content', '')}" for i, r in enumerate(results[:5])]
        )

        prompt = f"""æ ¹æ®ä»¥ä¸‹æœç´¢ç»“æœï¼Œç®€è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}

æœç´¢ç»“æœï¼š
{results_text}

è¯·ç”¨ 2-3 å¥è¯æ€»ç»“æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚"""

        # ä½¿ç”¨ LLM ç”Ÿæˆæ€»ç»“
        if self._mode == "mlx":
            from mlx_lm import generate

            loop = asyncio.get_event_loop()
            summary = await loop.run_in_executor(
                None,
                generate,
                self._mlx_model,
                self._mlx_tokenizer,
                prompt,
                256,  # max_tokens
            )
            return summary.strip()
        elif self._mode == "ollama":
            response = await self._ollama_client.generate(
                model=settings.ollama_model, prompt=prompt, options={"temperature": 0.7, "num_predict": 256}
            )
            return response["response"].strip()
        else:
            # Mock æ€»ç»“
            return f"æ ¹æ®æœç´¢ç»“æœï¼Œå…³äº '{query}' çš„ä¸»è¦ä¿¡æ¯å¦‚ä¸‹ï¼š{results[0].get('title', '')}ã€‚è¯¦è§æœç´¢ç»“æœã€‚"

    # MARK: - ç¼“å­˜è¾…åŠ©æ–¹æ³•ï¼ˆPhase 2 Week 4 Day 17ï¼‰

    def _generate_cache_key(self, query: str, num_results: int, language: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºæŸ¥è¯¢å‚æ•°çš„å“ˆå¸Œï¼‰"""
        key_string = f"{query}|{num_results}|{language}"
        return hashlib.md5(key_string.encode("utf-8")).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[List[Dict[str, Any]]]:
        """ä»ç¼“å­˜è·å–ç»“æœï¼ˆæ£€æŸ¥ TTLï¼‰"""
        if cache_key not in self._search_cache:
            return None

        cached_results, cached_time = self._search_cache[cache_key]

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ5 åˆ†é’Ÿ TTLï¼‰
        if time.time() - cached_time > self._cache_ttl:
            # è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜
            del self._search_cache[cache_key]
            return None

        return cached_results

    def _save_to_cache(self, cache_key: str, results: List[Dict[str, Any]]):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        self._search_cache[cache_key] = (results, time.time())

        # æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
        if len(self._search_cache) > 100:
            self._cleanup_expired_cache()

    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜æ¡ç›®"""
        current_time = time.time()
        expired_keys = [
            key
            for key, (_, cached_time) in self._search_cache.items()
            if current_time - cached_time > self._cache_ttl
        ]

        for key in expired_keys:
            del self._search_cache[key]

        logger.debug(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: åˆ é™¤ {len(expired_keys)} æ¡ï¼Œå‰©ä½™ {len(self._search_cache)} æ¡")
