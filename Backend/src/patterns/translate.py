#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright (c) 2026 Yu Geng. All rights reserved.
# MacCortex - Proprietary and Confidential

# MacCortex TranslatePattern - ç¿»è¯‘æ¨¡å¼
# Phase 1 - Week 2 Day 9
# åˆ›å»ºæ—¶é—´: 2026-01-20
# æ›´æ–°æ—¶é—´: 2026-01-21 (Phase 1.5 - Day 3: é›†æˆ PromptGuard)
#
# Phase 1.5: å¢å¼ºå®‰å…¨é˜²æŠ¤ï¼ˆPrompt Injection æ£€æµ‹ã€æŒ‡ä»¤éš”ç¦»ã€è¾“å‡ºæ¸…ç†ï¼‰
#
# å¤šè¯­è¨€ç¿»è¯‘ï¼ˆæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰ï¼‰

import asyncio
from typing import Any, Dict
from loguru import logger

from .base import BasePattern
from utils.config import settings
from utils.cache import TranslationCache  # Phase 3: ç¿»è¯‘ç¼“å­˜


class TranslatePattern(BasePattern):
    """
    ç¿»è¯‘ Pattern

    æ”¯æŒå¤šç§è¯­è¨€ä¹‹é—´çš„ç¿»è¯‘ï¼š
    - ä¸­æ–‡ â†” è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€æ³•æ–‡ã€å¾·æ–‡ã€è¥¿ç­ç‰™æ–‡ç­‰
    - è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
    - ä¿ç•™æ ¼å¼ä¸æœ¯è¯­
    - æ”¯æŒä¸“ä¸šæœ¯è¯­è¯å…¸
    """

    def __init__(self):
        super().__init__()  # Phase 1.5: åˆå§‹åŒ–å®‰å…¨æ¨¡å—
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        self._mode = "uninitialized"  # uninitialized | aya | mlx | ollama | mock
        self._aya_available = False  # Phase 3: aya-23 ç¿»è¯‘æ¨¡å‹å¯ç”¨æ€§

        # Phase 3 Backend ä¼˜åŒ–: ç¿»è¯‘ç¼“å­˜ï¼ˆLRU, 1000 æ¡ï¼‰
        self._cache = TranslationCache(max_size=1000, ttl_seconds=3600)  # 1 å°æ—¶è¿‡æœŸ

    # MARK: - BasePattern Protocol

    @property
    def pattern_id(self) -> str:
        return "translate"

    @property
    def name(self) -> str:
        return "Translate"

    @property
    def description(self) -> str:
        return "å¤šè¯­è¨€ç¿»è¯‘ï¼ˆæ”¯æŒä¸­è‹±æ—¥éŸ©æ³•å¾·è¥¿ç­‰ï¼‰"

    @property
    def version(self) -> str:
        return "1.0.0"

    async def initialize(self):
        """
        åˆå§‹åŒ–æ¨¡å‹ï¼ˆPhase 3: ä¼˜å…ˆä½¿ç”¨ aya-23 ç¿»è¯‘æ¨¡å‹ï¼‰

        ä¼˜å…ˆçº§é¡ºåºï¼š
        1. aya:8b (Ollama) - ä¸“ä¸šç¿»è¯‘æ¨¡å‹ï¼ˆPhase 3 æ–°å¢ï¼‰
        2. MLX Llama-3.2-1B - é€šç”¨æ¨¡å‹ï¼ˆè´¨é‡æœ‰é™ï¼‰
        3. Ollama é€šç”¨æ¨¡å‹ - å›é€€é€‰é¡¹
        4. Mock æ¨¡å¼ - æµ‹è¯•ç”¨
        """
        logger.info(f"ğŸ”§ åˆå§‹åŒ– {self.name} Pattern...")

        # Phase 3: ä¼˜å…ˆå°è¯• aya-23 ç¿»è¯‘æ¨¡å‹ï¼ˆOllamaï¼‰
        try:
            await self._initialize_aya()
            return  # aya æˆåŠŸï¼Œç›´æ¥è¿”å›
        except Exception as e:
            logger.info(f"  â„¹ï¸  aya æ¨¡å‹ä¸å¯ç”¨: {e}")

        # å›é€€ï¼šå°è¯•åŠ è½½ MLX æ¨¡å‹ï¼ˆApple Silicon ä¼˜åŒ–ï¼‰
        try:
            await self._initialize_mlx()
        except Exception as e:
            logger.warning(f"MLX åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ° Ollama: {e}")
            try:
                await self._initialize_ollama()
            except Exception as e2:
                logger.warning(f"Ollama åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ Mock æ¨¡å¼: {e2}")
                logger.info("  âš ï¸  ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰")

    async def _initialize_mlx(self):
        """åˆå§‹åŒ– MLX æ¨¡å‹"""
        try:
            import mlx.core as mx
            from mlx_lm import load

            logger.info(f"  ğŸ åŠ è½½ MLX æ¨¡å‹: {settings.mlx_model}")

            # å¼‚æ­¥åŠ è½½æ¨¡å‹ï¼ˆå¤ç”¨ SummarizePattern çš„æ¨¡å‹ï¼‰
            loop = asyncio.get_event_loop()
            self._mlx_model, self._mlx_tokenizer = await loop.run_in_executor(
                None, load, settings.mlx_model
            )

            self._mode = "mlx"
            logger.info("  âœ… MLX æ¨¡å‹åŠ è½½æˆåŠŸ")
        except ImportError:
            raise RuntimeError("MLX æœªå®‰è£…")
        except Exception as e:
            raise RuntimeError(f"MLX åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _initialize_aya(self):
        """
        åˆå§‹åŒ– aya-23 ç¿»è¯‘æ¨¡å‹ï¼ˆPhase 3 æ–°å¢ï¼‰

        aya-23 æ˜¯ Cohere å¼€å‘çš„ä¸“ä¸šå¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ï¼Œæ”¯æŒ 100+ è¯­è¨€ã€‚
        ç›¸æ¯” Llama-3.2-1Bï¼Œç¿»è¯‘è´¨é‡æå‡ 3-5 å€ã€‚

        ä¼˜å…ˆå°è¯•é¡ºåºï¼š
        1. aya:8b (~5 GB) - æ¨èï¼Œå¹³è¡¡æ€§èƒ½ä¸è´¨é‡
        2. aya:latest (aya-23, ~13 GB) - æœ€é«˜è´¨é‡
        """
        try:
            import ollama

            logger.info("  ğŸŒ æ£€æµ‹ aya ç¿»è¯‘æ¨¡å‹...")

            client = ollama.AsyncClient()

            # è·å–å·²å®‰è£…çš„æ¨¡å‹åˆ—è¡¨ï¼ˆPhase 3 Bug ä¿®å¤ï¼šollama è¿”å›å¯¹è±¡éå­—å…¸ï¼‰
            models_response = await client.list()
            installed_models = [m.model for m in models_response.models]

            # ä¼˜å…ˆä½¿ç”¨ aya:8bï¼ˆè½»é‡ç‰ˆï¼‰
            aya_model = None
            if any('aya:8b' in m for m in installed_models):
                aya_model = "aya:8b"
            elif any('aya' in m for m in installed_models):
                # ä½¿ç”¨ä»»ä½•å¯ç”¨çš„ aya æ¨¡å‹
                aya_model = next(m for m in installed_models if 'aya' in m)

            if not aya_model:
                raise RuntimeError("aya æ¨¡å‹æœªå®‰è£…ï¼ˆè¿è¡Œ: ollama pull aya:8bï¼‰")

            # æµ‹è¯•è¿æ¥
            logger.info(f"  ğŸŒ æµ‹è¯• aya æ¨¡å‹: {aya_model}")
            test_response = await client.generate(
                model=aya_model,
                prompt="Translate to English: ä½ å¥½",
                options={"num_predict": 10}
            )

            # Phase 3 Bug ä¿®å¤ï¼štest_response æ˜¯å¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®
            if not test_response.response:
                raise RuntimeError("aya æ¨¡å‹å“åº”ä¸ºç©º")

            # æˆåŠŸ
            self._ollama_client = client
            self._aya_available = True
            self._mode = "aya"
            logger.info(f"  âœ… aya ç¿»è¯‘æ¨¡å‹å°±ç»ª: {aya_model}")
            logger.info("     é¢„æœŸè´¨é‡æå‡: 3-5x vs Llama-3.2-1B")

        except ImportError:
            raise RuntimeError("Ollama æœªå®‰è£…")
        except Exception as e:
            raise RuntimeError(f"aya åˆå§‹åŒ–å¤±è´¥: {e}")

    async def _initialize_ollama(self):
        """åˆå§‹åŒ– Ollama å®¢æˆ·ç«¯"""
        try:
            import ollama

            logger.info(f"  ğŸ¦™ è¿æ¥ Ollama: {settings.ollama_model}")

            # æµ‹è¯•è¿æ¥
            client = ollama.AsyncClient()
            try:
                await client.generate(
                    model=settings.ollama_model, prompt="test", options={"num_predict": 1}
                )
                self._ollama_client = client
                self._mode = "ollama"
                logger.info("  âœ… Ollama è¿æ¥æˆåŠŸ")
            except Exception as e:
                raise RuntimeError(f"Ollama è¿æ¥å¤±è´¥: {e}")
        except ImportError:
            raise RuntimeError("Ollama æœªå®‰è£…")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self._mlx_model = None
        self._mlx_tokenizer = None
        self._ollama_client = None
        logger.info(f"âœ… {self.name} Pattern æ¸…ç†å®Œæˆ")

    async def execute(self, text: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œç¿»è¯‘

        Args:
            text: è¾“å…¥æ–‡æœ¬
            parameters: ç¿»è¯‘å‚æ•°
                - target_language: ç›®æ ‡è¯­è¨€ (å¿…å¡«, å¦‚ "en", "ja", "ko", "fr", "de", "es")
                - source_language: æºè¯­è¨€ (å¯é€‰, é»˜è®¤ "auto" è‡ªåŠ¨æ£€æµ‹)
                - style: ç¿»è¯‘é£æ ¼ (å¯é€‰, "formal"|"casual"|"technical", é»˜è®¤ "formal")
                - preserve_format: æ˜¯å¦ä¿ç•™æ ¼å¼ (é»˜è®¤: true)
                - glossary: æœ¯è¯­è¯å…¸ (å¯é€‰, Dict[str, str])

        Returns:
            ç¿»è¯‘ç»“æœå­—å…¸
        """
        # è§£æå‚æ•°
        target_language = parameters.get("target_language")
        if not target_language:
            raise ValueError("ç¼ºå°‘å¿…å¡«å‚æ•°: target_language")

        source_language = parameters.get("source_language", "auto")
        style = parameters.get("style", "formal")
        preserve_format = parameters.get("preserve_format", True)
        glossary = parameters.get("glossary", {})

        # Phase 3 Backend ä¼˜åŒ–: æ£€æŸ¥ç¼“å­˜
        cached_translation = self._cache.get(text, target_language, source_language, style)
        if cached_translation is not None:
            logger.info(
                f"ğŸš€ ç¼“å­˜å‘½ä¸­ | hit_rate={self._cache.hit_rate:.1%} | "
                f"text_preview={text[:30]}..."
            )
            return {
                "output": cached_translation,
                "metadata": {
                    "source_language": source_language,
                    "target_language": target_language,
                    "style": style,
                    "preserve_format": preserve_format,
                    "glossary_size": len(glossary),
                    "original_length": len(text),
                    "translation_length": len(cached_translation),
                    "mode": self._mode,
                    "cached": True,  # æ ‡è®°ä¸ºç¼“å­˜ç»“æœ
                    "cache_stats": self._cache.stats,  # ç¼“å­˜ç»Ÿè®¡
                },
            }

        # Phase 3: æ ¹æ®æ¨¡å¼é€‰æ‹©ç”Ÿæˆæ–¹æ³•ï¼ˆä¼˜å…ˆä½¿ç”¨ ayaï¼‰
        if self._mode == "aya":
            translation = await self._translate_with_aya(
                text, source_language, target_language, style, preserve_format, glossary
            )
        elif self._mode == "mlx":
            translation = await self._translate_with_mlx(
                text, source_language, target_language, style, preserve_format, glossary
            )
        elif self._mode == "ollama":
            translation = await self._translate_with_ollama(
                text, source_language, target_language, style, preserve_format, glossary
            )
        else:
            # Mock æ¨¡å¼
            translation = await self._translate_mock(
                text, source_language, target_language, style, preserve_format, glossary
            )

        # Phase 3 Backend ä¼˜åŒ–: å­˜å…¥ç¼“å­˜
        self._cache.put(text, target_language, translation, source_language, style)
        logger.debug(
            f"ç¼“å­˜å­˜å…¥ | cache_size={len(self._cache._cache)} | "
            f"hit_rate={self._cache.hit_rate:.1%}"
        )

        return {
            "output": translation,  # ç»Ÿä¸€è¾“å‡ºæ ¼å¼
            "metadata": {
                "source_language": source_language,
                "target_language": target_language,
                "style": style,
                "preserve_format": preserve_format,
                "glossary_size": len(glossary),
                "original_length": len(text),
                "translation_length": len(translation),
                "mode": self._mode,
                "cached": False,  # æ ‡è®°ä¸ºæ–°ç¿»è¯‘
                "cache_stats": self._cache.stats,  # ç¼“å­˜ç»Ÿè®¡
            },
        }

    async def _translate_with_mlx(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        preserve_format: bool,
        glossary: Dict[str, str],
    ) -> str:
        """ä½¿ç”¨ MLX æ¨¡å‹è¿›è¡Œç¿»è¯‘"""
        from mlx_lm import generate

        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(text, source_language, target_language, style, preserve_format, glossary)

        # ç”Ÿæˆï¼ˆåŒæ­¥æ–¹æ³•ï¼Œéœ€è¦åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰
        loop = asyncio.get_event_loop()
        output = await loop.run_in_executor(
            None,
            generate,
            self._mlx_model,
            self._mlx_tokenizer,
            prompt,
            1024,  # max_tokens
        )

        # æå–ç¿»è¯‘ç»“æœ
        return self._extract_translation(output)

    async def _translate_with_aya(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        preserve_format: bool,
        glossary: Dict[str, str],
    ) -> str:
        """
        ä½¿ç”¨ aya-23 è¿›è¡Œç¿»è¯‘ï¼ˆPhase 3 æ–°å¢ï¼‰

        aya-23 æ˜¯ä¸“ä¸šå¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ï¼Œç›¸æ¯” Llama-3.2-1B æœ‰æ˜¾è‘—æå‡ï¼š
        - æ”¯æŒ 100+ è¯­è¨€
        - ç¿»è¯‘è´¨é‡æå‡ 3-5 å€
        - æ›´å‡†ç¡®çš„è¯­ä¹‰ç†è§£
        - æ›´å¥½çš„æ ¼å¼ä¿ç•™
        """
        # è·å– aya æ¨¡å‹åç§°ï¼ˆPhase 3 Bug ä¿®å¤ï¼šollama è¿”å›å¯¹è±¡éå­—å…¸ï¼‰
        models_response = await self._ollama_client.list()
        installed_models = [m.model for m in models_response.models]
        aya_model = next((m for m in installed_models if 'aya' in m), "aya:8b")

        # æ„å»ºä¼˜åŒ–çš„ aya æç¤ºè¯ï¼ˆaya æ¨¡å‹ç‰¹å®šä¼˜åŒ–ï¼‰
        prompt = self._build_aya_prompt(text, source_language, target_language, style, preserve_format, glossary)

        # ç”Ÿæˆï¼ˆaya æ¨¡å‹æ¨èå‚æ•°ï¼‰
        response = await self._ollama_client.generate(
            model=aya_model,
            prompt=prompt,
            options={
                "temperature": 0.3,  # ä½æ¸©åº¦ç¡®ä¿ç¿»è¯‘å‡†ç¡®æ€§
                "num_predict": min(len(text) * 3, 2048),  # åŠ¨æ€ token é™åˆ¶
                "top_p": 0.9,
                "repeat_penalty": 1.1,  # é¿å…é‡å¤
            }
        )

        # æå–ç¿»è¯‘ç»“æœï¼ˆPhase 3 Bug ä¿®å¤ï¼šresponse æ˜¯å¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®ï¼‰
        translation = self._extract_translation(response.response)

        # aya ç‰¹æ®Šæ¸…ç†ï¼šç§»é™¤å¯èƒ½çš„å…ƒæ•°æ®
        if translation.startswith("[") and "]" in translation:
            # ç§»é™¤ [è¯­è¨€] å‰ç¼€
            translation = translation.split("]", 1)[-1].strip()

        return translation

    async def _translate_with_ollama(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        preserve_format: bool,
        glossary: Dict[str, str],
    ) -> str:
        """ä½¿ç”¨ Ollama è¿›è¡Œç¿»è¯‘"""
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(text, source_language, target_language, style, preserve_format, glossary)

        # ç”Ÿæˆ
        response = await self._ollama_client.generate(
            model=settings.ollama_model, prompt=prompt, options={"temperature": 0.5, "num_predict": 1024}
        )

        # æå–ç¿»è¯‘ç»“æœï¼ˆPhase 3 Bug ä¿®å¤ï¼šresponse æ˜¯å¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®ï¼‰
        return self._extract_translation(response.response)

    async def _translate_mock(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        preserve_format: bool,
        glossary: Dict[str, str],
    ) -> str:
        """Mock æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        await asyncio.sleep(0.1)

        # ç®€å•çš„ Mock ç¿»è¯‘
        mock_translations = {
            "zh-CN_en": "This is a mock translation from Chinese to English.",
            "en_zh-CN": "è¿™æ˜¯ä»è‹±æ–‡åˆ°ä¸­æ–‡çš„æ¨¡æ‹Ÿç¿»è¯‘ã€‚",
            "zh-CN_ja": "ã“ã‚Œã¯ä¸­å›½èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ãƒ¢ãƒƒã‚¯ç¿»è¨³ã§ã™ã€‚",
            "zh-CN_ko": "ì´ê²ƒì€ ì¤‘êµ­ì–´ì—ì„œ í•œêµ­ì–´ë¡œì˜ ëª¨ì˜ ë²ˆì—­ì…ë‹ˆë‹¤.",
            "zh-CN_fr": "Ceci est une traduction simulÃ©e du chinois vers le franÃ§ais.",
            "zh-CN_de": "Dies ist eine Mock-Ãœbersetzung vom Chinesischen ins Deutsche.",
            "zh-CN_es": "Esta es una traducciÃ³n simulada del chino al espaÃ±ol.",
        }

        # è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€ï¼ˆç®€å•åˆ¤æ–­ï¼‰
        if source_language == "auto":
            # æ£€æµ‹æ˜¯å¦æœ‰ä¸­æ–‡å­—ç¬¦
            has_chinese = any("\u4e00" <= c <= "\u9fff" for c in text)
            source_language = "zh-CN" if has_chinese else "en"

        key = f"{source_language}_{target_language}"
        translation = mock_translations.get(key, f"[Mock ç¿»è¯‘] åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")

        # æ·»åŠ é£æ ¼æ ‡è®°
        if style == "formal":
            translation = f"[æ­£å¼é£æ ¼] {translation}"
        elif style == "casual":
            translation = f"[éšæ„é£æ ¼] {translation}"
        elif style == "technical":
            translation = f"[æŠ€æœ¯é£æ ¼] {translation}"

        return translation

    def _build_prompt(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        preserve_format: bool,
        glossary: Dict[str, str],
    ) -> str:
        """
        æ„å»ºç¿»è¯‘æç¤ºè¯ï¼ˆPhase 2 Week 4 Day 16 ä¼˜åŒ–ï¼‰

        ä¼˜åŒ–è¦ç‚¹ï¼š
        1. ç®€åŒ–ç³»ç»Ÿæç¤ºï¼Œæ˜ç¡®ä»»åŠ¡ç›®æ ‡
        2. å¼ºè°ƒ"åªè¾“å‡ºç¿»è¯‘"ï¼Œé¿å…é‡å¤åŸæ–‡
        3. ä½¿ç”¨ IMPORTANT æ ‡è®°å…³é”®è§„åˆ™
        """
        # è¯­è¨€ä»£ç æ˜ å°„ï¼ˆPhase 2 Week 4 Day 16ï¼šåŒæ—¶æ”¯æŒç®€çŸ­å’Œå®Œæ•´æ ¼å¼ï¼‰
        lang_names = {
            "auto": "è‡ªåŠ¨æ£€æµ‹",
            # ä¸­æ–‡
            "zh": "ç®€ä½“ä¸­æ–‡",
            "zh-CN": "ç®€ä½“ä¸­æ–‡",
            "zh-TW": "ç¹ä½“ä¸­æ–‡",
            # è‹±æ–‡
            "en": "English",
            "en-US": "English",
            # æ—¥æ–‡
            "ja": "æ—¥æœ¬èª",
            "ja-JP": "æ—¥æœ¬èª",
            # éŸ©æ–‡
            "ko": "í•œêµ­ì–´",
            "ko-KR": "í•œêµ­ì–´",
            # æ³•æ–‡
            "fr": "FranÃ§ais",
            "fr-FR": "FranÃ§ais",
            # å¾·æ–‡
            "de": "Deutsch",
            "de-DE": "Deutsch",
            # è¥¿ç­ç‰™æ–‡
            "es": "EspaÃ±ol",
            "es-ES": "EspaÃ±ol",
            # ä¿„æ–‡
            "ru": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            "ru-RU": "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
            # é˜¿æ‹‰ä¼¯æ–‡
            "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
            "ar-AR": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        }

        target_name = lang_names.get(target_language, target_language)

        # é£æ ¼æè¿°æ˜ å°„ï¼ˆæ›´ç®€æ´ï¼‰
        style_map = {
            "formal": "formal and professional",
            "casual": "casual and conversational",
            "technical": "technical and precise"
        }
        style_desc = style_map.get(style, "natural")

        # æ ¹æ®ç›®æ ‡è¯­è¨€é€‰æ‹©ç³»ç»Ÿæç¤ºè¯­è¨€ï¼ˆPhase 2 Week 4 Day 16 ä¼˜åŒ– v2ï¼‰
        # ä¸ºä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰ç›®æ ‡è¯­è¨€ä½¿ç”¨ä¸­æ–‡ç³»ç»Ÿæç¤ºï¼Œå¢å¼ºæ¨¡å‹ç†è§£
        use_chinese_prompt = target_language in ["zh", "zh-CN", "zh-TW", "ja", "ja-JP", "ko", "ko-KR"]

        if use_chinese_prompt:
            # ä¸­æ–‡ç³»ç»Ÿæç¤ºï¼ˆé’ˆå¯¹äºšæ´²è¯­è¨€ç¿»è¯‘ä¼˜åŒ–ï¼‰
            prompt = f"""ä½ æ˜¯ä¸“ä¸šç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_name}ï¼ˆ{style_desc} é£æ ¼ï¼‰ã€‚

é‡è¦è§„åˆ™ï¼š
- åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦è§£é‡Š
- ä¸è¦é‡å¤åŸæ–‡
- ä¿ç•™åŸæ–‡çš„è¯­æ°”å’Œæ„æ€"""

            if preserve_format:
                prompt += "\n- ä¿ç•™åŸæ–‡æ ¼å¼ï¼ˆæ¢è¡Œã€æ®µè½ã€æ ‡ç‚¹ï¼‰"

            if glossary:
                glossary_str = ", ".join([f"{k}â†’{v}" for k, v in glossary.items()])
                prompt += f"\n- ä½¿ç”¨è¿™äº›æœ¯è¯­ï¼š{glossary_str}"

            # ç”¨æˆ·å†…å®¹ï¼ˆæ˜ç¡®åˆ†éš”ï¼‰
            prompt += f"\n\nåŸæ–‡ï¼š\n{text}\n\nç¿»è¯‘ç»“æœï¼š"
        else:
            # è‹±æ–‡ç³»ç»Ÿæç¤ºï¼ˆé’ˆå¯¹è¥¿æ–¹è¯­è¨€ç¿»è¯‘ï¼‰
            prompt = f"""You are a professional translator.
Translate the following text to {target_name} ({style_desc} style).

IMPORTANT:
- Only output the translation, NO explanations
- Do NOT repeat the original text
- Preserve the meaning and tone"""

            if preserve_format:
                prompt += "\n- Keep the formatting (line breaks, paragraphs, punctuation)"

            if glossary:
                glossary_str = ", ".join([f"{k}â†’{v}" for k, v in glossary.items()])
                prompt += f"\n- Use these terms: {glossary_str}"

            # ç”¨æˆ·å†…å®¹ï¼ˆæ˜ç¡®åˆ†éš”ï¼‰
            prompt += f"\n\nText to translate:\n{text}\n\nTranslation:"

        return prompt

    def _build_aya_prompt(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        preserve_format: bool,
        glossary: Dict[str, str],
    ) -> str:
        """
        æ„å»º aya-23 ä¸“ç”¨ç¿»è¯‘æç¤ºè¯ï¼ˆPhase 3 æ–°å¢ï¼‰

        aya-23 æ¨¡å‹ç‰¹æ€§ä¼˜åŒ–ï¼š
        1. åŸç”Ÿæ”¯æŒ 100+ è¯­è¨€ï¼Œæ— éœ€å¤æ‚è¯­è¨€æ˜ å°„
        2. æ›´æ“…é•¿ç†è§£ç®€æ´ç›´æ¥çš„æŒ‡ä»¤
        3. è‡ªå¸¦è¯­è¨€æ£€æµ‹èƒ½åŠ›ï¼Œsource_language å¯é€‰
        4. æ›´å¥½çš„æ ¼å¼ä¿ç•™èƒ½åŠ›

        æç¤ºè¯è®¾è®¡åŸåˆ™ï¼š
        - ä½¿ç”¨è‹±æ–‡æŒ‡ä»¤ï¼ˆaya æ¨¡å‹è®­ç»ƒä¼˜åŒ–ï¼‰
        - ç®€æ´æ˜ç¡®çš„ä»»åŠ¡æè¿°
        - å¼ºè°ƒ"ç›´æ¥è¾“å‡ºç¿»è¯‘"
        - åˆ©ç”¨ aya çš„å¤šè¯­è¨€ç†è§£ä¼˜åŠ¿
        """
        # è¯­è¨€ä»£ç æ˜ å°„ï¼ˆaya åŸç”Ÿæ”¯æŒæ ‡å‡† ISO 639-1 ä»£ç ï¼‰
        lang_names = {
            "auto": "detected language",
            # ç®€åŒ–æ˜ å°„ï¼šaya æ”¯æŒæ ‡å‡†ä»£ç 
            "zh": "Chinese",
            "zh-CN": "Simplified Chinese",
            "zh-TW": "Traditional Chinese",
            "en": "English",
            "en-US": "English",
            "ja": "Japanese",
            "ja-JP": "Japanese",
            "ko": "Korean",
            "ko-KR": "Korean",
            "fr": "French",
            "fr-FR": "French",
            "de": "German",
            "de-DE": "German",
            "es": "Spanish",
            "es-ES": "Spanish",
            "ru": "Russian",
            "ru-RU": "Russian",
            "ar": "Arabic",
            "ar-AR": "Arabic",
            "pt": "Portuguese",
            "pt-BR": "Brazilian Portuguese",
            "it": "Italian",
            "nl": "Dutch",
            "pl": "Polish",
            "tr": "Turkish",
            "vi": "Vietnamese",
            "th": "Thai",
            "id": "Indonesian",
            "hi": "Hindi",
        }

        target_name = lang_names.get(target_language, target_language)
        source_name = lang_names.get(source_language, source_language)

        # é£æ ¼æè¿°ï¼ˆaya æ›´ç†è§£è‹±æ–‡æŒ‡ä»¤ï¼‰
        style_map = {
            "formal": "formal and professional",
            "casual": "casual and conversational",
            "technical": "technical and precise"
        }
        style_desc = style_map.get(style, "natural")

        # aya ä¸“ç”¨ç®€æ´æç¤ºè¯ï¼ˆåŸºäº Cohere æ¨èæ ¼å¼ï¼‰
        if source_language == "auto":
            # æ— æºè¯­è¨€ï¼Œä¾èµ– aya çš„è‡ªåŠ¨æ£€æµ‹
            prompt = f"""Translate this text to {target_name} ({style_desc} style).

Rules:
- Output ONLY the translation
- NO explanations or comments
- Preserve meaning and tone"""
        else:
            # æ˜ç¡®æºè¯­è¨€ï¼ˆæé«˜å‡†ç¡®æ€§ï¼‰
            prompt = f"""Translate from {source_name} to {target_name} ({style_desc} style).

Rules:
- Output ONLY the translation
- NO explanations or comments
- Preserve meaning and tone"""

        # æ ¼å¼ä¿ç•™ï¼ˆaya æ“…é•¿ï¼‰
        if preserve_format:
            prompt += "\n- Keep original formatting (line breaks, paragraphs, punctuation)"

        # æœ¯è¯­è¯å…¸ï¼ˆaya çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¼ºï¼‰
        if glossary:
            glossary_str = ", ".join([f'"{k}" â†’ "{v}"' for k, v in glossary.items()])
            prompt += f"\n- Use these terms: {glossary_str}"

        # ç”¨æˆ·å†…å®¹ï¼ˆæ¸…æ™°åˆ†éš”ï¼‰
        prompt += f"\n\nText:\n{text}\n\nTranslation:"

        return prompt

    def _extract_translation(self, output: str) -> str:
        """ä»æ¨¡å‹è¾“å‡ºä¸­æå–ç¿»è¯‘ç»“æœ"""
        # å»é™¤å¯èƒ½çš„å‰ç¼€/åç¼€è¯´æ˜
        output = output.strip()

        # ç§»é™¤å¸¸è§çš„å‰ç¼€æ¨¡å¼
        prefixes = ["ç¿»è¯‘ç»“æœï¼š", "Translation:", "è¯‘æ–‡ï¼š", "Result:"]
        for prefix in prefixes:
            if output.startswith(prefix):
                output = output[len(prefix) :].strip()

        # ç§»é™¤ä»£ç å—æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰
        if output.startswith("```") and output.endswith("```"):
            lines = output.split("\n")
            output = "\n".join(lines[1:-1])

        return output.strip()

    # MARK: - Streaming Support (Phase 3 Week 3)

    async def execute_stream(self, text: str, parameters: Dict[str, Any]):
        """
        æµå¼ç¿»è¯‘ï¼ˆServer-Sent Eventsï¼‰

        Phase 3 Week 3 Day 1 æ–°å¢åŠŸèƒ½
        è¿”å› StreamingResponseï¼Œé€å­—å‘é€ç¿»è¯‘ç»“æœï¼ˆç±»ä¼¼ ChatGPT æ‰“å­—æ•ˆæœï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬
            parameters: ç¿»è¯‘å‚æ•°ï¼ˆåŒ executeï¼‰

        Returns:
            StreamingResponseï¼ˆtext/event-streamï¼‰
        """
        from fastapi.responses import StreamingResponse
        import json

        async def event_generator():
            """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
            try:
                # 1. å‘é€å¼€å§‹äº‹ä»¶
                yield f"event: start\n"
                yield f"data: {json.dumps({'status': 'started', 'input_length': len(text)})}\n\n"
                await asyncio.sleep(0.01)  # ç¡®ä¿äº‹ä»¶é¡ºåº

                # 2. è§£æå‚æ•°
                target_language = parameters.get("target_language")
                if not target_language:
                    yield f"event: error\n"
                    yield f"data: {json.dumps({'error': 'ç¼ºå°‘å¿…å¡«å‚æ•°: target_language'})}\n\n"
                    return

                source_language = parameters.get("source_language", "auto")
                style = parameters.get("style", "formal")

                # 3. æ£€æŸ¥ç¼“å­˜
                cached_translation = self._cache.get(text, target_language, source_language, style)

                if cached_translation is not None:
                    # ç¼“å­˜å‘½ä¸­ï¼šæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœå‘é€ç¼“å­˜ç»“æœ
                    yield f"event: cached\n"
                    yield f"data: {json.dumps({'cached': True, 'hit_rate': self._cache.hit_rate})}\n\n"

                    # é€å­—å‘é€ï¼ˆæ¯æ¬¡ 5 ä¸ªå­—ç¬¦ï¼Œ50ms å»¶è¿Ÿï¼‰
                    chunk_size = 5
                    for i in range(0, len(cached_translation), chunk_size):
                        chunk = cached_translation[i:i+chunk_size]
                        yield f"event: chunk\n"
                        yield f"data: {json.dumps({'text': chunk})}\n\n"
                        await asyncio.sleep(0.05)  # 50ms æ‰“å­—å»¶è¿Ÿ

                    # å‘é€å®Œæˆäº‹ä»¶ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
                    metadata = {
                        "source_language": source_language,
                        "target_language": target_language,
                        "style": style,
                        "cached": True,
                        "cache_stats": self._cache.stats,
                        "mode": self._mode,
                    }
                    yield f"event: done\n"
                    yield f"data: {json.dumps({'output': cached_translation, 'metadata': metadata})}\n\n"

                else:
                    # ç¼“å­˜æœªå‘½ä¸­ï¼šæµå¼ç¿»è¯‘
                    yield f"event: translating\n"
                    yield f"data: {json.dumps({'cached': False})}\n\n"

                    # æ ¹æ®æ¨¡å¼é€‰æ‹©æµå¼æ–¹æ³•
                    if self._mode == "aya":
                        async for chunk in self._translate_stream_aya(
                            text, source_language, target_language, style, parameters
                        ):
                            yield f"event: chunk\n"
                            yield f"data: {json.dumps({'text': chunk})}\n\n"
                    else:
                        # é aya æ¨¡å¼ï¼šå›é€€åˆ°æ™®é€šç¿»è¯‘ + æ¨¡æ‹Ÿæµå¼
                        logger.warning("æµå¼ç¿»è¯‘ä»…æ”¯æŒ aya æ¨¡å¼ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæµå¼")
                        translation = await self.execute(text, parameters)
                        full_text = translation["output"]

                        # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                        chunk_size = 5
                        for i in range(0, len(full_text), chunk_size):
                            chunk = full_text[i:i+chunk_size]
                            yield f"event: chunk\n"
                            yield f"data: {json.dumps({'text': chunk})}\n\n"
                            await asyncio.sleep(0.05)

                    # è·å–å®Œæ•´ç¿»è¯‘ç»“æœï¼ˆç”¨äºç¼“å­˜ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é‡æ–°è°ƒç”¨ execute æ¥è·å–å…ƒæ•°æ®å’Œç¼“å­˜
                    # ä½†ä¸ºé¿å…é‡å¤ç¿»è¯‘ï¼Œæˆ‘ä»¬ç›´æ¥æ„é€ å…ƒæ•°æ®
                    full_translation = ""  # ä» chunk ç´¯ç§¯

                    # TODO: æ”¹è¿›ï¼šåœ¨æµå¼è¿‡ç¨‹ä¸­ç´¯ç§¯å®Œæ•´æ–‡æœ¬
                    # å½“å‰å®ç°ï¼šå‘é€å®Œæˆäº‹ä»¶ä½†ä¸åŒ…å«å®Œæ•´æ–‡æœ¬ï¼ˆå®¢æˆ·ç«¯è‡ªè¡Œç´¯ç§¯ï¼‰
                    yield f"event: done\n"
                    yield f"data: {json.dumps({'metadata': {'mode': self._mode, 'cached': False}})}\n\n"

            except Exception as e:
                logger.error(f"æµå¼ç¿»è¯‘é”™è¯¯: {e}")
                yield f"event: error\n"
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
            }
        )

    async def _translate_stream_aya(
        self,
        text: str,
        source_language: str,
        target_language: str,
        style: str,
        parameters: Dict[str, Any],
    ):
        """
        ä½¿ç”¨ aya æ¨¡å‹è¿›è¡Œæµå¼ç¿»è¯‘

        Phase 3 Week 3 Day 1 æ–°å¢
        åˆ©ç”¨ Ollama çš„æµå¼ API å®æ—¶ç”Ÿæˆç¿»è¯‘

        Yields:
            str: ç¿»è¯‘æ–‡æœ¬ç‰‡æ®µ
        """
        # è·å– aya æ¨¡å‹åç§°
        models_response = await self._ollama_client.list()
        installed_models = [m.model for m in models_response.models]
        aya_model = next((m for m in installed_models if 'aya' in m), "aya:8b")

        # æ„å»ºæç¤ºè¯
        preserve_format = parameters.get("preserve_format", True)
        glossary = parameters.get("glossary", {})
        prompt = self._build_aya_prompt(
            text, source_language, target_language, style, preserve_format, glossary
        )

        # è°ƒç”¨ Ollama æµå¼ API
        full_text = ""
        async for part in await self._ollama_client.generate(
            model=aya_model,
            prompt=prompt,
            stream=True,  # å¯ç”¨æµå¼
            options={
                "temperature": 0.3,
                "num_predict": min(len(text) * 3, 2048),
                "top_p": 0.9,
                "repeat_penalty": 1.1,
            }
        ):
            # ollama æµå¼å“åº”æ ¼å¼ï¼š{response: str, done: bool}
            if hasattr(part, 'response'):
                chunk = part.response
            elif isinstance(part, dict):
                chunk = part.get('response', '')
            else:
                chunk = str(part)

            if chunk:
                full_text += chunk
                yield chunk

        # æµå¼å®Œæˆåï¼šå­˜å…¥ç¼“å­˜
        if full_text:
            cleaned_text = self._extract_translation(full_text)
            self._cache.put(text, target_language, cleaned_text, source_language, style)
            logger.debug(f"æµå¼ç¿»è¯‘å®Œæˆï¼Œå·²å­˜å…¥ç¼“å­˜ | length={len(cleaned_text)}")
